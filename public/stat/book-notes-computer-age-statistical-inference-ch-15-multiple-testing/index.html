<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.88.1" />


<title>Book Notes: Computer Age Statistical Inference -- Ch15 Multiple Testing - A Hugo website</title>
<meta property="og:title" content="Book Notes: Computer Age Statistical Inference -- Ch15 Multiple Testing - A Hugo website">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
    <li><a href="https://twitter.com/rstudio">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Book Notes: Computer Age Statistical Inference -- Ch15 Multiple Testing</h1>

    

    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#classical-before-computer-age-multiple-testing-corrections">Classical (Before Computer Age) Multiple Testing Corrections</a><ul>
<li><a href="#bonferroni-correction">Bonferroni Correction</a></li>
<li><a href="#family-wise-error-rate">Family-wise Error Rate</a></li>
</ul></li>
<li><a href="#false-discovery-rates">False Discovery Rates</a><ul>
<li><a href="#benjamini-hochberg-fdr-control">Benjamini-Hochberg FDR control</a></li>
<li><a href="#an-empirical-bayes-view">An empirical Bayes view</a></li>
</ul></li>
<li><a href="#local-false-discovery-rates">Local False Discovery Rates</a></li>
<li><a href="#empirical-null">Empirical Null</a></li>
</ul>
</div>

<p><strong><em><font color='red'>For the pdf slides, click <a href="/pdf/092020_computer_age_stat_inference_Part3_Ch15.pdf">here</a></font></em></strong></p>
<div id="classical-before-computer-age-multiple-testing-corrections" class="section level1">
<h1>Classical (Before Computer Age) Multiple Testing Corrections</h1>
<div id="background-and-notations" class="section level3">
<h3>Background and notations</h3>
<ul>
<li><p>Before computer age, multiple testing may only involve 10 or 20 tests.
With the emerge of biomedical (microarray) data,
multiple testing may need to evaluate several thousands of tests</p></li>
<li><p>Notations</p>
<ul>
<li><span class="math inline">\(N\)</span>: total number of tests, e.g., number of genes.</li>
<li><span class="math inline">\(z_i\)</span>: the z-statistic of the <span class="math inline">\(i\)</span>-th test.
Note that if we perform tests other than z-test, say a t-test,
then we can use inverse-cdf method to transform the t-statistic into a z-statistic, like below
<span class="math display">\[
  z_i = \Phi^{-1}\left[F_{df}(t_i)\right], 
  \]</span>
where <span class="math inline">\(\Phi\)</span> is the standard normal cdf, and <span class="math inline">\(F\)</span> is a t distribution cdf.</li>
<li><span class="math inline">\(I_0\)</span>: the indices of the true <span class="math inline">\(H_{0i}\)</span>, having <span class="math inline">\(N_0\)</span> members.
Usually, majority of hypotheses are null, so <span class="math inline">\(\pi_0 = N_0/N\)</span> is close to 1.</li>
</ul></li>
<li><p>Hypotheses: standard normal vs normal with a non-zero mean
<span class="math display">\[
  H_{0i}: z_i \sim \text{N}(0, 1) \longleftrightarrow H_{1i}: z_i \sim \text{N}(\mu_i, 1)
  \]</span>
where <span class="math inline">\(\mu_i\)</span> is the effect size for test <span class="math inline">\(i\)</span></p></li>
</ul>
</div>
<div id="example-the-prostate-data" class="section level3">
<h3>Example: the prostate data</h3>
<ul>
<li><p>A microarray data of</p>
<ul>
<li><span class="math inline">\(n=102\)</span> people, 52 prostate cancer patients and 50 normal controls</li>
<li><span class="math inline">\(N = 6033\)</span> genes</li>
</ul></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="/figures/CASI_fig_15_1.png" alt="Histogram of 6033 z-values, with the scaled standard normal density curve in red" width="70%" />
<p class="caption">
Figure 1: Histogram of 6033 z-values, with the scaled standard normal density curve in red
</p>
</div>
</div>
<div id="bonferroni-correction" class="section level2">
<h2>Bonferroni Correction</h2>
<div id="classical-multiple-testing-method-1-bonferroni-bound" class="section level3">
<h3>Classical multiple testing method 1: <font color='blue'>Bonferroni bound</font></h3>
<ul>
<li>For an overall significance level <span class="math inline">\(\alpha\)</span> (usually <span class="math inline">\(\alpha = 0.05\)</span>), with <span class="math inline">\(N\)</span> simultaneous tests,
the <font color='blue'>Bonferroni bound</font> rejects the <span class="math inline">\(i\)</span>th null hypothesis <span class="math inline">\(H_{0i}\)</span> at individual significance level</li>
</ul>
<p><span class="math display">\[p_i \leq \frac{\alpha}{N}\]</span></p>
<ul>
<li><p><font color='red'>Bonferroni bound is quite conservative!</font></p>
<ul>
<li>For prostate data <span class="math inline">\(N=6033\)</span> and <span class="math inline">\(\alpha = 0.05\)</span>, the <span class="math inline">\(p\)</span>-value rejection cutoff is very small:
<span class="math inline">\(p_i \leq 8.3\times 10^{-6}\)</span></li>
</ul></li>
</ul>
</div>
</div>
<div id="family-wise-error-rate" class="section level2">
<h2>Family-wise Error Rate</h2>
<div id="classical-multiple-testing-method-2-fwer-control" class="section level3">
<h3>Classical multiple testing method 2: <font color='blue'>FWER control</font></h3>
<ul>
<li><p>The <font color='blue'>family-wise error rate</font> is the probability of making even one false rejection
<span class="math display">\[
\text{FWER} = P(\text{reject any true } H_{0i})
\]</span></p></li>
<li><p>Bonferroni’s procedure controls FWER, i.e., <font color='red'>Bonferroni bound is more conservative than FWER control</font>
<span class="math display">\[\begin{align*}
\text{FWER} 
&amp;= P\left\{\cup_{i \in I_0}\left(p_i \leq \frac{\alpha}{N}\right)\right\}
\leq \sum_{i \in I_0}P\left(p_i \leq \frac{\alpha}{N}\right)\\
&amp;= N_0 \frac{\alpha}{N} \leq \alpha
\end{align*}\]</span></p></li>
</ul>
</div>
<div id="fwer-control-holms-procedure" class="section level3">
<h3>FWER control: <font color='blue'>Holm’s procedure</font></h3>
<ol style="list-style-type: decimal">
<li><p>Order the observed <span class="math inline">\(p\)</span>-values from smallest to largest
<span class="math display">\[
p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(i)} \ldots \leq p_{(N)} 
\]</span></p></li>
<li><p>Reject null hypotheses <span class="math inline">\(H_{0(i)}\)</span> if
<span class="math display">\[
p_{(i)} \leq \text{Threshold(Holm&#39;s)} = \frac{\alpha}{N-i+1}
\]</span></p></li>
</ol>
<ul>
<li><font color='red'>FWER is usually still too conservative for large <span class="math inline">\(N\)</span>, since it was originally developed for</font> <span class="math inline">\(N\leq 20\)</span></li>
</ul>
</div>
<div id="an-r-function-to-implement-holms-procedure" class="section level3">
<h3>An R function to implement Holm’s procedure</h3>
<pre class="r"><code>## A function to obtain Holm&#39;s procedure p-value cutoff
holm = function(pi, alpha=0.1){
  N = length(pi)
  idx = order(pi)
  reject = which(pi[idx] &lt;= alpha/(N - 1:N + 1))
  
  return(idx[reject])
}</code></pre>
<pre class="r"><code>## Download prostate data&#39;s z-values
link = &#39;https://web.stanford.edu/~hastie/CASI_files/DATA/prostz.txt&#39;
prostz = c(read.table(link))$V1
## Convert to p-values
prostp = 1 - pnorm(prostz)</code></pre>
</div>
<div id="illustrate-holms-procedure-on-the-prostate-data" class="section level3">
<h3>Illustrate Holm’s procedure on the prostate data</h3>
<pre class="r"><code>## Apply Holm&#39;s procedure on the prostate data
results = holm(prostp)
## Total number of rejected null hypotheses
r = length(results); r</code></pre>
<pre><code>## [1] 6</code></pre>
<pre class="r"><code>## The largest z-value among non-rejected nulls
sort(prostz, decreasing = TRUE)[r + 1]</code></pre>
<pre><code>## [1] 4.13538</code></pre>
<pre class="r"><code>## The smallest p-value among non-rejected nulls
sort(prostp)[r + 1]</code></pre>
<pre><code>## [1] 1.771839e-05</code></pre>
</div>
</div>
</div>
<div id="false-discovery-rates" class="section level1">
<h1>False Discovery Rates</h1>
<div id="false-discovery-proportion" class="section level3">
<h3>False discovery proportion</h3>
<ul>
<li><p>FDR control is a more liberal criterion (compared with FWER),
thus it has become standard for large <span class="math inline">\(N\)</span> multiple testing problems.</p></li>
<li><font color='blue'>False discovery proportion</font>
<span class="math display">\[
\text{Fdp}(\mathcal{D}) = 
\begin{cases}
a/R, &amp; {\text{if }} R \neq 0\\
0, &amp; {\text{if }} R = 0
\end{cases}
\]</span>
<ul>
<li>A decision rule <span class="math inline">\(\mathcal{D}\)</span> rejects <span class="math inline">\(R\)</span> out of <span class="math inline">\(N\)</span> null hypotheses</li>
<li><span class="math inline">\(a\)</span> of those are false discoveries (unobservable)</li>
</ul></li>
</ul>
<p><img src="/figures/CASI_fig_15_2.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="false-discovery-rate" class="section level3">
<h3>False discovery rate</h3>
<ul>
<li><p><font color='blue'>False discovery rates</font>
<span class="math display">\[
\text{FDR}(\mathcal{D}) = E\{\text{Fdp}(\mathcal{D})\}
\]</span></p></li>
<li><p>A decision rule <span class="math inline">\(\mathcal{D}\)</span> controls FDR at level <span class="math inline">\(q\)</span>, if
<span class="math display">\[
\text{FDR}(\mathcal{D}) \leq q
\]</span></p>
<ul>
<li><span class="math inline">\(q\)</span> is a prechosen value between 0 and 1</li>
</ul></li>
</ul>
</div>
<div id="benjamini-hochberg-fdr-control" class="section level2">
<h2>Benjamini-Hochberg FDR control</h2>
<div id="benjamini-hochberg-fdr-control-1" class="section level3">
<h3><font color='blue'>Benjamini-Hochberg FDR control</font></h3>
<ol style="list-style-type: decimal">
<li><p>Order the observed <span class="math inline">\(p\)</span>-values from smallest to largest
<span class="math display">\[
p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(i)} \ldots \leq p_{(N)} 
\]</span></p></li>
<li><p>Reject null hypotheses <span class="math inline">\(H_{0(i)}\)</span> if
<span class="math display">\[
p_{(i)} \leq \text{Threshold}(\mathcal{D}_q) = \frac{q}{N} i
\]</span></p></li>
</ol>
<ul>
<li><p>Default choice <span class="math inline">\(q = 0.1\)</span></p></li>
<li><p><font color='green'>Theorem: if the <span class="math inline">\(p\)</span>-values are independent of each other, then the above procedure controls FDR at level <span class="math inline">\(q\)</span>, i.e.,</font>
<span class="math display">\[
\text{FDR}(\mathcal{D}_q) = \pi_0 q \leq q, \quad \text{where } \pi_0 = N_0 / N
\]</span></p>
<ul>
<li>Usually, majority of the hypotheses are truly null, so <span class="math inline">\(\pi_0\)</span> is near 1</li>
</ul></li>
</ul>
</div>
<div id="an-r-function-to-implement-benjamini-hochberg-fdr-control" class="section level3">
<h3>An R function to implement Benjamini-Hochberg FDR control</h3>
<pre class="r"><code>## A function to obtain Holm&#39;s procedure p-value cutoff
bh = function(pi, q=0.1){
  N = length(pi)
  idx = order(pi)
  reject = which(pi[idx] &lt;= q/N * (1:N))
  
  return(idx[reject])
}</code></pre>
</div>
<div id="illustrate-benjamini-hochberg-fdr-control-on-the-prostate-data" class="section level3">
<h3>Illustrate Benjamini-Hochberg FDR control on the prostate data</h3>
<pre class="r"><code>## Apply Holm&#39;s procedure on the prostate data
results = bh(prostp)
## Total number of rejected null hypotheses
r = length(results); r</code></pre>
<pre><code>## [1] 28</code></pre>
<pre class="r"><code>## The largest z-value among non-rejected nulls
sort(prostz, decreasing = TRUE)[r + 1]</code></pre>
<pre><code>## [1] 3.293507</code></pre>
<pre class="r"><code>## The smallest p-value among non-rejected nulls
sort(prostp)[r + 1]</code></pre>
<pre><code>## [1] 0.0004947302</code></pre>
</div>
<div id="comparing-holms-fwer-control-and-benjamini-hochberg-fdr-control" class="section level3">
<h3>Comparing Holm’s FWER control and Benjamini-Hochberg FDR control</h3>
<ul>
<li><p>In the usual range of interest, large <span class="math inline">\(N\)</span> and small <span class="math inline">\(i\)</span>, the ratio
<span class="math display">\[
\frac{\text{Threshold}(\mathcal{D}_q)}{\text{Threshold(Holm&#39;s)}}
= \frac{q}{\alpha}\left(1 - \frac{i-1}{N}\right) i
\]</span>
increases with <span class="math inline">\(i\)</span> almost linearly</p></li>
<li><p>The figure below is about the prostate data, with <span class="math inline">\(\alpha = q = 0.1\)</span></p></li>
</ul>
<p><img src="/figures/CASI_fig_15_3.png" width="65%" style="display: block; margin: auto;" /></p>
</div>
<div id="question-about-the-fdr-control-procedure" class="section level3">
<h3><font color='red'>Question about the FDR control procedure</font></h3>
<ol style="list-style-type: decimal">
<li><p>Is controlling a rate (i.e., FDR) as meaningful as controlling a probability (of Type 1 error)?</p></li>
<li><p>How should <span class="math inline">\(q\)</span> be chosen?</p></li>
<li><p>The control theorem depends on independence among the <span class="math inline">\(p\)</span>-values. What if they’re dependent, which is usually the case?</p></li>
<li><p>The FDR significance for one gene depends on the results of all other genes. Does this make sense?</p></li>
</ol>
</div>
</div>
<div id="an-empirical-bayes-view" class="section level2">
<h2>An empirical Bayes view</h2>
<div id="two-groups-model" class="section level3">
<h3><font color='blue'>Two-groups model</font></h3>
<ul>
<li><p>Each of the <span class="math inline">\(N\)</span> cases (e.g., genes) is</p>
<ul>
<li>either null with prior probability <span class="math inline">\(\pi_0\)</span>,</li>
<li>or non-null with probability <span class="math inline">\(\pi_1 = 1- \pi_0\)</span></li>
</ul></li>
<li><p>For case <span class="math inline">\(i\)</span>, its <span class="math inline">\(z\)</span>-value <span class="math inline">\(z_i\)</span> under <span class="math inline">\(H_{ij}\)</span> for <span class="math inline">\(j = 0, 1\)</span> has density <span class="math inline">\(f_j(z)\)</span>, cdf <span class="math inline">\(F_j(z)\)</span>, and survival curve
<span class="math display">\[S_j(z) = 1 - F_j(z)\]</span></p></li>
<li><p>The mixture survival curve
<span class="math display">\[\begin{align*}
S(z) = \pi_0 S_0(z) + \pi_1 S_1(z)
\end{align*}\]</span></p></li>
</ul>
</div>
<div id="bayesian-false-discovery-rate" class="section level3">
<h3>Bayesian false-discovery rate</h3>
<ul>
<li><p>Suppose the observation <span class="math inline">\(z_i\)</span> for case <span class="math inline">\(i\)</span> is seen to exceed some threshold value <span class="math inline">\(z_0\)</span> (say <span class="math inline">\(z_0 = 3\)</span>).
By Bayes’ rule, the <font color='blue'>Bayesian false-discovery rate</font> is
<span class="math display">\[\begin{align*}
\text{Fdr}(z_0) &amp; = P(\text{case } i \text{ is null} \mid z_i \geq z_0)\\
&amp;= \frac{\pi_0 S_0(z_0)}{S(z_0)}
\end{align*}\]</span></p></li>
<li><p>The “empirical” Bayes reflects in the estimation of the denominator: when <span class="math inline">\(N\)</span> is large,
<span class="math display">\[
\hat{S}(z_0) = \frac{N(z_0)}{N}, \quad N(z_0) = \#\{z_i \geq z_0\}
\]</span></p></li>
<li><p>An empirical Bayes estimate of the Bayesian false-discovery rate
<span class="math display">\[
\widehat{\text{Fdr}}(z_0) = \frac{\pi_0 S_0(z_0)}{\hat{S}(z_0)}
\]</span></p></li>
</ul>
</div>
<div id="connection-between-widehattextfdr-and-fdr-controls" class="section level3">
<h3>Connection between <span class="math inline">\(\widehat{\text{Fdr}}\)</span> and FDR controls</h3>
<ul>
<li><p>Since <span class="math inline">\(p_i = S_0(z_i)\)</span> and <span class="math inline">\(\hat{S}(z_{(i)}) = i/N\)</span>, the FDR control <span class="math inline">\(\mathcal{D}_q\)</span> algorithm
<span class="math display">\[
p_{(i)} \leq \frac{i}{N}\cdot q
\]</span>
becomes
<span class="math display">\[
S_0(z_{(i)}) \leq \hat{S}(z_{(i)}) \cdot q,
\]</span>
After rearranging the above formula, we have its Bayesian Fdr bounded
<span class="math display">\[\begin{equation}\label{eq:Fdr}
\widehat{\text{Fdr}}(z_0) \leq \pi_0 q
\end{equation}\]</span></p></li>
<li><p><font color='green'>The FDR control algorithm is in fact rejecting those cases for which the empirical Bayes posterior probability of nullness is too small</font></p></li>
</ul>
</div>
<div id="answer-the-4-questions-about-the-fdr-control" class="section level3">
<h3><font color='green'>Answer the 4 questions about the FDR control</font></h3>
<ol style="list-style-type: decimal">
<li><p>(Rate vs probability) FDR control does relate to the posterior probability of nullness</p></li>
<li><p>(Choice of <span class="math inline">\(q\)</span>) We can set <span class="math inline">\(q\)</span> according to the maximum tolerable amount of Bayes risk of nullness, usually after taking <span class="math inline">\(\pi_0 = 1\)</span> in </p></li>
<li><p>(Independence) Most often the <span class="math inline">\(z_i\)</span>, and hence the <span class="math inline">\(p_i\)</span>, are correlated.
However even under correlation,
<span class="math inline">\(\hat{S}(z_0)\)</span> is still an unbiased estimator for <span class="math inline">\(S_(z_0)\)</span>,
making <span class="math inline">\(\widehat{\text{Fdr}}(z_0)\)</span> nearly unbiased for <span class="math inline">\(\text{Fdr}(z_0)\)</span>.</p>
<ul>
<li>There is a price to be paid for correlation,
which increases the <em>variance</em> of <span class="math inline">\(\hat{S}(z_0)\)</span> and <span class="math inline">\(\widehat{\text{Fdr}}(z_0)\)</span></li>
</ul></li>
<li><p>(Rejecting one test depending on others) In the Bayes two-group model,
the number of null cases <span class="math inline">\(z_i\)</span> exceeding some threshold <span class="math inline">\(z_0\)</span> has <em>fixed</em> expectation <span class="math inline">\(N \pi_0 S_0(z_0)\)</span>.
So an increase in the number of <span class="math inline">\(z_i\)</span> exceeding <span class="math inline">\(z_0\)</span> must come from a heavier right tail for <span class="math inline">\(f_1(z)\)</span>,
implying a greater posterior probability of non-nullness <span class="math inline">\(\text{Fdr}(z_0)\)</span>.</p>
<ul>
<li>This emphasizes the “learning from the experience of others” aspect of empirical Bayes inference</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div id="local-false-discovery-rates" class="section level1">
<h1>Local False Discovery Rates</h1>
<div id="local-false-discovery-rates-1" class="section level3">
<h3>Local false discovery rates</h3>
<ul>
<li><p>Having observed test statistic <span class="math inline">\(z_i\)</span> equal to some value <span class="math inline">\(z_0\)</span>,
we should be more interested in the probability of nullness given <span class="math inline">\(z_i = z_0\)</span> than <span class="math inline">\(z_i \geq z_0\)</span></p></li>
<li><p><font color='blue'>Local false discovery rate</font>
<span class="math display">\[\begin{align*}
\text{fdr}(z_0) 
&amp;= P(\text{case } i \text{ is null} \mid z_i = z_0)\\
&amp; = \frac{\pi_0 f_0(z_0)}{f(z_0)}
\end{align*}\]</span></p></li>
<li><p>After drawing a smooth curve <span class="math inline">\(\hat{f}(z)\)</span> through the histogram of the <span class="math inline">\(z\)</span>-values,
we get the estimate
<span class="math display">\[
\widehat{\text{fdr}}(z_0) = \frac{\pi_0 f_0(z_0)}{\hat{f}(z_0)}
\]</span></p>
<ul>
<li>the null proportion <span class="math inline">\(\pi_0\)</span> can either be estimated or set equal to 1</li>
</ul></li>
</ul>
</div>
<div id="a-fourth-degree-log-polynomial-poisson-regression-fit-to-the-histogram-on-the-prostate-data" class="section level3">
<h3>A fourth-degree log polynomial Poisson regression fit to the histogram, on the prostate data</h3>
<ul>
<li><p>Solid line is the local <span class="math inline">\(\widehat{\text{fdr}}(z)\)</span> and dashed lines are tail-area <span class="math inline">\(\widehat{\text{Fdr}}(z)\)</span></p></li>
<li><p>27 genes on the right and 25 one the left have <span class="math inline">\(\widehat{\text{fdr}}(z_i) \leq 0.2\)</span></p></li>
</ul>
<p><img src="/figures/CASI_fig_15_5.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="the-default-cutoff-for-local-fdr" class="section level3">
<h3>The default cutoff for local fdr</h3>
<ul>
<li><p>The cutoff <span class="math inline">\(\widehat{\text{fdr}}(z_i) \leq 0.2\)</span> is equivalent to
<span class="math display">\[
\frac{f_1(z)}{f_0(z)} \geq 4 \frac{\pi_0}{\pi_1}
\]</span></p></li>
<li><p>Assuming <span class="math inline">\(\pi_0 \geq 0.9\)</span>, this makes the factor factor quite large
<span class="math display">\[
\frac{f_1(z)}{f_0(z)} \geq 36
\]</span>
This is “strong evidence” against the null hypothesis in Jeffrey’s scale of evidence for the interpretation of Bayes factors</p></li>
</ul>
<p><img src="/figures/CASI_tab_13_3.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="relation-between-the-local-and-tail-area-fdrs" class="section level3">
<h3>Relation between the local and tail-area fdr’s</h3>
<ul>
<li><p>Since
<span class="math display">\[
\text{Fdr}(z_0) = E\left(\text{fdr}(z) \mid z \geq z_0  \right)
\]</span>
Therefore
<span class="math display">\[
\text{Fdr}(z_0) &lt; \text{fdr}(z_0)
\]</span></p></li>
<li><p>Thus, the conventional significant cutoffs are
<span class="math display">\[\begin{align*}
\widehat{\text{Fdr}}(z) &amp; \leq 0.1\\
\widehat{\text{fdr}}(z) &amp; \leq 0.2
\end{align*}\]</span></p></li>
</ul>
</div>
</div>
<div id="empirical-null" class="section level1">
<h1>Empirical Null</h1>
<div id="empirical-null-1" class="section level3">
<h3><font color='blue'>Empirical null</font></h3>
<ul>
<li><p>Large scale applications may allow us to empirically determine a more realistic null distribution than
<span class="math inline">\(H_{0i}: z_i \sim \text{N}(0, 1)\)</span></p></li>
<li><p>In the police data, a <span class="math inline">\(\text{N}(0, 1)\)</span> curve is too narrow for the null.
Actually, an MLE fit to central data gives <span class="math inline">\(\text{N}(0.10, 1.40^2)\)</span> as the empirical null</p></li>
</ul>
<p><img src="/figures/CASI_fig_15_7.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="empirical-null-estimation" class="section level3">
<h3>Empirical null estimation</h3>
<ul>
<li><p>The theoretical null <span class="math inline">\(z_i \sim \text{N}(0,1)\)</span> is not completely wrong, but needs adjustment for the dataset at hand</p></li>
<li><p>Under the two-group model, with <span class="math inline">\(f_0(z)\)</span> normal but not necessarily standard normal
<span class="math display">\[
f_0(z) \sim \text{N}(\delta_0, \sigma_0^2),
\]</span>
to compute the local <span class="math inline">\(\text{fdr}(z) = \pi_0 f_0(z) / f(z)\)</span>, we need to estimate three parameters
<span class="math inline">\((\delta_0, \sigma_0, \pi_0)\)</span></p></li>
<li><p>Our key assumption is that <span class="math inline">\(\pi_0\)</span> is large, say <span class="math inline">\(\pi_0 \geq 0.9\)</span>, and most of the <span class="math inline">\(z_i\)</span> near <span class="math inline">\(0\)</span> are null.</p></li>
<li><p>The algorithm <a href="https://cran.r-project.org/web/packages/locfdr/index.html"><code>locfdr</code></a> begins by selecting a set <span class="math inline">\(\mathcal{A}_0\)</span> near <span class="math inline">\(z=0\)</span> and assumes that all the <span class="math inline">\(z_i\)</span> in <span class="math inline">\(\mathcal{A}_0\)</span> are null</p></li>
<li><p>Maximum likelihood based on the numbers and values of <span class="math inline">\(z_i\)</span> in <span class="math inline">\(\mathcal{A}_0\)</span> yield the empirical null estimates <span class="math inline">\((\hat{\delta}_0, \hat{\sigma}_0, \hat{\pi}_0)\)</span></p></li>
</ul>
<!-- 
### A comparison among all methods

|Large-scale testing method      |Reject criterion      |Default parameters     |Reject region (one sided) for prostate data ($N = 6033$)|
|----------------------------|-------------------------|------------------------|----------------------------------------|
|Bonferroni | $p_i < \frac{\alpha}{N}$  | $\alpha = 0.05$ |  $p_i < 8.3\times 10^{-6}$, $z_i > 4.31$                    |          |----------------------------------|----------------------------------|----------------------------------|------------------------------------------------|
| FWER          |   $p_{(i)} < \frac{\alpha}{N-i+1}$                   |   $\alpha = 0.05??$                    |    $p_i < 1.1\times 10^{-5}$, $z_i > 4.25$                                    |
| FDR           |   $p_{(i)} < \frac{q}{N} i$                   |       $q = 0.1$                |                                        |
|            |                      |                       |                                        |
|            |                      |                       |                                        |
 -->
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><p>Efron, Bradley and Hastie, Trevor (2016), <em>Computer Age Statistical Inference</em>.
Cambridge University Press</p></li>
<li><p>Links to the prostate data</p>
<ul>
<li>The <span class="math inline">\(6033 \times 102\)</span> data matrix: <a href="https://web.stanford.edu/~hastie/CASI_files/DATA/prostmat.csv"><em>prostmat.csv</em></a></li>
<li>The <span class="math inline">\(6033\)</span> z-values: <a href="https://web.stanford.edu/~hastie/CASI_files/DATA/prostz.txt"><em>prostz.txt</em></a></li>
</ul></li>
<li><p>A list of FDR methods in R: <a href="http://www.strimmerlab.org/notes/fdr.html" class="uri">http://www.strimmerlab.org/notes/fdr.html</a></p></li>
</ul>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

