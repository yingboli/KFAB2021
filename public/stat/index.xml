<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Notes on Statistics and Machine Learning Books, Courses, Papers, and More on King Fox And Butterfly</title>
    <link>http://liyingbo.com/stat/</link>
    <description>Recent content in My Notes on Statistics and Machine Learning Books, Courses, Papers, and More on King Fox And Butterfly</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://liyingbo.com/stat/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Tabular Deep Learning</title>
      <link>http://liyingbo.com/stat/2025/02/04/tabular-neural-networks/</link>
      <pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2025/02/04/tabular-neural-networks/</guid>
      <description>Empirical Comparisms between Neural Networks (NN) and Gradient Boosted Decision Trees (GBDT) Tabular Deep Learning  Neural Networks (Non Transformer-Based)  Transformer-Based GBDT Inspired Preprocessing References   It has almost been 10 years since modern implementations of Gradient Boosted Decision Trees (GBDT), such as XGBoost [REF], LightGBM [REF], and CatBoost [REF], have become the industry standard machine learning methods for predictive modeling (classification and regression), thanks to their superior predictive accuracy and computation efficiency.</description>
    </item>
    
    <item>
      <title>Course Notes: A Crash Course on Causality -- Week 5: Instrumental Variables</title>
      <link>http://liyingbo.com/stat/2021/08/16/course-notes-a-crash-course-on-causality-week-5-instrumental-variables/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/08/16/course-notes-a-crash-course-on-causality-week-5-instrumental-variables/</guid>
      <description>Introduction to Instrumental Variables Randomized trials with noncompliance Compliance classes Instrumental variable assumptions  Estimate Causal Effects with Instrumental Variables IVs in observational studies Two stage least squares Sensitivity analysis and weak instruments    For the pdf slides, click here
Introduction to Instrumental Variables Unmeasured confounding  Suppose there are unobserved variables \(U\) that affect both \(A\) and \(Y\), then \(U\) is an unmeasured confounding   This violates ignorability assumption</description>
    </item>
    
    <item>
      <title>Course Notes: A Crash Course on Causality -- Week 4: Inverse Probability of Treatment Weighting (IPTW)</title>
      <link>http://liyingbo.com/stat/2021/07/29/course-notes-a-crash-course-on-causality-week-4-inverse-probability-of-treatment-weighting-iptw/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/07/29/course-notes-a-crash-course-on-causality-week-4-inverse-probability-of-treatment-weighting-iptw/</guid>
      <description>Inverse Probability of Treatment Weighting Marginal Structural Models Assessing covariate balance with weights Problems and remedies for large weights    For the pdf slides, click here
Inverse Probability of Treatment Weighting Motivating example  Suppose there is a single confounder \(X\), with propensity scores \[ P(A=1\mid X=1) = 0.1, \quad P(A=1\mid X=0) = 0.8 \]   In propensity score matching, for subjects with \(X=1\), 1 out of 9 controls will be matched to the treated</description>
    </item>
    
    <item>
      <title>Course Notes: A Crash Course on Causality -- Week 3: Matching and Propensity Scores</title>
      <link>http://liyingbo.com/stat/2021/07/26/course-notes-a-crash-course-on-causality-week-3-matching-and-propensity-scores/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/07/26/course-notes-a-crash-course-on-causality-week-3-matching-and-propensity-scores/</guid>
      <description>Matching Experiments vs observational studies Overview of matching Nearest neighbor matching Optimal matching Assessing matching balance Analyze data after matching  Propensity Score Propensity score matching    For the pdf slides, click here
Matching Experiments vs observational studies Randomized trials  In experiments, treatment \(A\) is determined by a coin toss; so there are no arrows from \(X\) to \(A\), i.e., no backdoor paths
 Covariate balance: distribution of pre-treatment variables \(X\) are the same in both treatment groups</description>
    </item>
    
    <item>
      <title>Course Notes: A Crash Course on Causality -- Week 2: Confounding and Directed Acyclic Graphs (DAGs)</title>
      <link>http://liyingbo.com/stat/2021/07/09/course-notes-a-crash-course-on-causality-week-2-confounding-and-directed-acyclic-graphs-dags/</link>
      <pubDate>Fri, 09 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/07/09/course-notes-a-crash-course-on-causality-week-2-confounding-and-directed-acyclic-graphs-dags/</guid>
      <description>Confounding Causal Graphs Terminologies of Directed Acyclic Graphs (DAGs) Relationship between DAGs and probability distributions Types of paths, blocking, and colliders d-separation Frontdoor and backdoor paths Backdoor path criterion    For the pdf slides, click here
Confounding Confounding  Confounders: variables that affect both the treatment and the outcome
 If we assign treatment based on a coin flip, since the coin flip doesn’t affect the outcome, it’s not a confounder</description>
    </item>
    
    <item>
      <title>Course Notes: A Crash Course on Causality -- Week 1: Intro to Causal Effects</title>
      <link>http://liyingbo.com/stat/2021/07/05/course-notes-a-crash-course-on-causality-week-1-intro-to-causal-effects/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/07/05/course-notes-a-crash-course-on-causality-week-1-intro-to-causal-effects/</guid>
      <description>Notations and Terminologies Potential outcomes and conterfactuals  What are Causal Effects? Fundamental problem of causal inference  Causal Assumptions Stable Unit Treatment Value Assumption (SUTVA) Consistency Ignorability Positivity  Standardization and Stratification   For the pdf slides, click here
Notations and Terminologies Notations  We are interested in the causal effect of some treatment \(A\) on some outcome \(Y\)
 Treatment: \(A\), binary
 \(A=1\) if receive treatment; and \(A=0\) if receive control</description>
    </item>
    
    <item>
      <title>Book Notes: Neural Network Methods for Natural Language Processing -- Part 3 Specialized Architectures, Ch14-16 RNNs</title>
      <link>http://liyingbo.com/stat/2021/05/18/book-notes-neural-network-methods-for-natural-language-processing-part-3-specialized-architectures-ch14-16-rnns/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/05/18/book-notes-neural-network-methods-for-natural-language-processing-part-3-specialized-architectures-ch14-16-rnns/</guid>
      <description>Ch14 Recurrent Neural Networks: Modeling Sequences and Stacks The RNN Abstraction Common RNN usages Bidirectional RNNs and Deep RNNs  Ch15 Concrete Recurrent Neural Network Architectures Simple RNN Gated Architectures: LSTM and GRU  Ch16 Modeling with Recurrent Networks Sentiment Classification    For the pdf slides, click here
Ch14 Recurrent Neural Networks: Modeling Sequences and Stacks RNNs overview  RNNs allow representing arbitrarily sized sequential inputs in fixed-sized vectors, while paying attention to the structured properties of the inputs</description>
    </item>
    
    <item>
      <title>Book Notes: Neural Network Methods for Natural Language Processing -- Part 3 Specialized Architectures, Ch13 CNN</title>
      <link>http://liyingbo.com/stat/2021/05/17/book-notes-neural-network-methods-for-natural-language-processing-part-3-specialized-architectures-ch13-cnn/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/05/17/book-notes-neural-network-methods-for-natural-language-processing-part-3-specialized-architectures-ch13-cnn/</guid>
      <description>Ch13 Ngram Detectors: Convolutional Neural Networks CNN Overivew Basic Convolution \(+\) Pooling Hierarchical Convolutions    For the pdf slides, click here
Overview on CNN and RNN for NLP  CNN and RNN architectures explored in this part of the book are primarily used as feature extractors
 CNNs and RNNs as Lego bricks: one just needs to make sure that input and output dimensions of the different components match</description>
    </item>
    
    <item>
      <title>Book Notes: Generalized Additive Models -- Ch4 Introducing GAMs</title>
      <link>http://liyingbo.com/stat/2021/03/27/book-notes-generalized-additive-models-ch4-introducing-gams/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/03/27/book-notes-generalized-additive-models-ch4-introducing-gams/</guid>
      <description>Univariate Smoothing Piecewise linear basis: tent functions Penalty to control wiggliness  Additive Models Generalized Additive Models Introducing Package mgcv   For the pdf slides, click here
Introduction of GAM  In general the GAM model has a following structure \[ g(\mu_i) = \mathbf{A}_i \boldsymbol\theta + f_1 (x_{1i}) + f_2 (x_{2i}) + f_3 (x_{3i}, x_{4i}) + \cdots \]
 \(Y_i\) follows some exponential family distribution: \(Y_i \sim EF(\mu_i, \phi)\) \(\mu_i = E(Y_i)\) \(\mathbf{A}_i\) is a row of the model matrix, and \(\boldsymbol\theta\) is the corresponding parameter vector \(f_j\) are smooth functions of the covariates \(x_k\)  This chapter  Illustrates GAMs by basis expansions, each with a penalty controlling function smoothness Estimates GAMs by penalized regression methods  Takeaway: technically GAMs are simply GLM estimated subject to smoothing penalties</description>
    </item>
    
    <item>
      <title>Book Notes: Generalized Additive Models - Ch3 Generalized Linear Models (GLM)</title>
      <link>http://liyingbo.com/stat/2021/03/21/book-notes-generalized-additive-models-ch3-generalized-linear-models-glm/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/03/21/book-notes-generalized-additive-models-ch3-generalized-linear-models-glm/</guid>
      <description>Theory of GLMs Exponential family Iteratively re-weighted least square (IRLS) Asymptotic consistency of MLE, deviance, tests, residuals Quasi-likelihood (GEE)  Generalized Linear Mixed Models (GLMM)   For the pdf slides, click here
GLM overview  In a GLM, a smooth monotonic link function \(g(\cdot)\) connects the expectation \(\mu_i = E(Y_i)\) with the linear combination of \(\mathbf{X}_i\), \[\begin{equation}\label{eq:glm_link} g(\mu_i) = \eta_i = \mathbf{X}_i \boldsymbol\beta \end{equation}\]
 In a generalized linear mixed model (GLMM), we have \[ g(\mu_i) = \eta_i = \mathbf{X}_i \boldsymbol\beta + \mathbf{Z}_i \mathbf{b}, \quad \mathbf{b} \sim \text{N}(\mathbf{0}, \boldsymbol\psi) \]</description>
    </item>
    
    <item>
      <title>Book Notes: Gaussian Processes for Machine learning -- Ch2 Gaussian Process Regression</title>
      <link>http://liyingbo.com/stat/2021/03/17/book-notes-gaussian-processes-for-machine-learning-ch2-gaussian-process-regression/</link>
      <pubDate>Wed, 17 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/03/17/book-notes-gaussian-processes-for-machine-learning-ch2-gaussian-process-regression/</guid>
      <description>Weight-space View Function-space View Prediction with noise-free observations Prediction with noisy observations Cholesky decomposition and GP regression algorithm Hyperparameters  Smoothing, Weight Functions, and Equivalent Kernels   For the pdf slides, click here
Overview of Gaussian processes (GP)  The problem is learning in GP is exactly the problem of finding suitable properties for the covariance function
 In this book, design matrix is defined slightly differently from common statistical textbooks.</description>
    </item>
    
    <item>
      <title>Book Notes: Neural Network Methods for Natural Language Processing -- Part 2 Working with Natural Language Data, Ch9-11</title>
      <link>http://liyingbo.com/stat/2021/01/22/book-notes-neural-network-methods-for-natural-language-processing-part-2-working-with-natural-language-data-ch9-11/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/01/22/book-notes-neural-network-methods-for-natural-language-processing-part-2-working-with-natural-language-data-ch9-11/</guid>
      <description>Ch9 Language Modeling Language Modeling with the Markov Assumption Neural Language Models  Ch10 Pre-trained Word Representations Word Simiarlity Matrices and SVD Word2Vec Model Choice of Contexts  Ch11 Using Word Embeddings Resources of Common Pre-Training Word Embeddings Usages: Find Similarity, Word Analogies    For the pdf slides, click here
Ch9 Language Modeling Language Modeling with the Markov Assumption Language modeling with the Markov assumption  The task of language modeling is to assign a probability to any sequence of words \(w_{1:n}\), i.</description>
    </item>
    
    <item>
      <title>Book Notes: Neural Network Methods for Natural Language Processing -- Part 2 Working with Natural Language Data, Ch6-8</title>
      <link>http://liyingbo.com/stat/2021/01/21/book-notes-neural-network-methods-for-natural-language-processing-part-2-working-with-natural-language-data-ch6-8/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/01/21/book-notes-neural-network-methods-for-natural-language-processing-part-2-working-with-natural-language-data-ch6-8/</guid>
      <description>Ch6 Features for Textural Data Preprocessing TF-IDF Weighting Ngrams  Ch7 Case Studies of NLP Features NLP Features for Document Topic Classification  Ch8 From Textual Features to Inputs Embeddings Combining Dense Vectors: sum, concat, CBOW Odds and Ends    For the pdf slides, click here
Ch6 Features for Textural Data Preprocessing Feature extraction  The mapping from textural data to real valued vectors is called feature extraction or feature representation</description>
    </item>
    
    <item>
      <title>Book Notes: Neural Network Methods for Natural Language Processing -- Part 1 Supervised Classification and Feed-forward Neural Networks</title>
      <link>http://liyingbo.com/stat/2021/01/20/book-notes-neural-network-methods-for-natural-language-processing-part-1-supervised-classification-and-feed-forward-neural-networks/</link>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/01/20/book-notes-neural-network-methods-for-natural-language-processing-part-1-supervised-classification-and-feed-forward-neural-networks/</guid>
      <description>Ch1 Introduction Ch2 Linear Models Ch4 Feed-Forward Neural Networks Ch5 Neural Network Training   For the pdf slides, click here
Ch1 Introduction Three most common types of NNs in NLP  Feed-forward networks, i.e., multi-layer perceptrons (MLPs), or fully connected layers
 Allow to work with fixed sized inputs Or with variable length inputs in which we can disregard the order of the elements (continuous bags of words)  Recurrent neural networks (RNNs)</description>
    </item>
    
    <item>
      <title>Book Notes: Pattern Recognition and Machine Learning -- Ch10 Variational Inference</title>
      <link>http://liyingbo.com/stat/2020/10/27/book-notes-pattern-recognition-and-machine-learning-ch10-variational-inference/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/10/27/book-notes-pattern-recognition-and-machine-learning-ch10-variational-inference/</guid>
      <description>Variational Inference Introduction of the variational inference method Example: univariate Gaussian Model selection  Variational Mixture of Gaussians Variational Linear Regression Exponential Family Distributions Local Variational Methods Variational Logistic Regression Expectation Propagation   For the pdf slides, click here
Variational Inference Introduction of the variational inference method Definitions  Variational inference is also called variational Bayes, thus  all parameters are viewed as random variables, and they will have prior distributions.</description>
    </item>
    
    <item>
      <title>Paper Notes: Proper Scoring Rules and Cost Weighted Loss Functions for Binary Classification</title>
      <link>http://liyingbo.com/stat/2020/10/12/paper-notes-proper-scoring-rules-and-cost-weighted-loss-functions-for-binary-classification/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/10/12/paper-notes-proper-scoring-rules-and-cost-weighted-loss-functions-for-binary-classification/</guid>
      <description>Proper Scoring Rules Commonly Used Proper Scoring Rules Structure of Proper Scoring Rules Proper scoring rules are mixtures of cost-weighted misclassification losses Beta Family of Proper Scoring Rules Examples   For the pdf slides, click here
Notations: in binary classification  We are interested in fitting a model \(q(\mathbf{x})\) for the true conditional class 1 probability \[ \eta(\mathbf{x}) = P(Y = 1 \mid \mathbf{X} = \mathbf{x}) \]</description>
    </item>
    
    <item>
      <title>Paper Notes: Generalized R Squared</title>
      <link>http://liyingbo.com/stat/2020/10/05/paper-notes-generalized-r-squared/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/10/05/paper-notes-generalized-r-squared/</guid>
      <description>Generalized \(R^2\) by Cox and Snell Generalized \(R^2\) by Nagelkerke Generalized \(R^2\) for binary data    For the pdf slides, click here
\(R^2\) for normal linear regression  \(R^2\), also called coefficient of determination or multiple correlation coefficient, is defined for normal linear regression, as the proportion of variance “explained” by the regression model \[\begin{equation}\label{eq:R2} R^2 = \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \bar{y} \right)^2} \end{equation}\]</description>
    </item>
    
    <item>
      <title>Book Notes: Computer Age Statistical Inference -- Ch15 Multiple Testing</title>
      <link>http://liyingbo.com/stat/2020/09/28/book-notes-computer-age-statistical-inference-ch-15-multiple-testing/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/09/28/book-notes-computer-age-statistical-inference-ch-15-multiple-testing/</guid>
      <description>Classical (Before Computer Age) Multiple Testing Corrections Bonferroni Correction Family-wise Error Rate  False Discovery Rates Benjamini-Hochberg FDR control An empirical Bayes view  Local False Discovery Rates Empirical Null   For the pdf slides, click here
Classical (Before Computer Age) Multiple Testing Corrections Background and notations  Before computer age, multiple testing may only involve 10 or 20 tests. With the emerge of biomedical (microarray) data, multiple testing may need to evaluate several thousands of tests</description>
    </item>
    
    <item>
      <title>Book Notes: Statistical Analysis with Missing Data -- Ch3 Complete Case Analysis and Weighting Methods</title>
      <link>http://liyingbo.com/stat/2020/09/08/book-notes-statistical-analysis-with-missing-data-ch3-complete-case-analysis-and-weighting-methods/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/09/08/book-notes-statistical-analysis-with-missing-data-ch3-complete-case-analysis-and-weighting-methods/</guid>
      <description>Weighted Complete-Case Analysis Available-Case Analysis   For the pdf slides, click here
Complete-case (CC) analysis  Complete-case (CC) analysis: use only data points (units) where all variables are observed
 Loss of information in CC analysis:
 Loss of precision (larger variance) Bias, when the missingness mechanism is not MCAR. In this case, the complete units are not a random sample of the population  In this notes, I will focus on the bias issue</description>
    </item>
    
    <item>
      <title>Book Notes: Flexible Imputation of Missing Data -- Ch4 Multivariate Missing Data</title>
      <link>http://liyingbo.com/stat/2020/08/25/book-notes-flexible-imputation-of-missing-data-ch4-multivariate-missing-data/</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/08/25/book-notes-flexible-imputation-of-missing-data-ch4-multivariate-missing-data/</guid>
      <description>Missing Data Pattern Fully Conditional Specification (FCS)   For the pdf slides, click here
Notations in this chapter  \(Y\): a \(n \times p\) matrix which contains is missing data \(Y_j\): the \(j\)th column in \(Y\) \(Y_{-j}\): all but the \(j\)th column of \(Y\) \(R\): a \(n \times p\) missing indicator matrix  \(0\) is missing and \(1\) is observed    Missing Data Pattern Missing data pattern summary statistics  When the number of columns is small, we can use the md.</description>
    </item>
    
    <item>
      <title>Book Notes: Flexible Imputation of Missing Data -- Ch3 Univariate Missing Data</title>
      <link>http://liyingbo.com/stat/2020/08/24/book-notes-flexible-imputation-of-missing-data-ch3-univariate-missing-data/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/08/24/book-notes-flexible-imputation-of-missing-data-ch3-univariate-missing-data/</guid>
      <description>Imputation under the Normal Linear Model Predictive Mean Matching Imputation under CART Imputing Categorical and Other Types of Data   For the pdf slides, click here
Notations  In this chapter, we assume that there is only one variable having missing values. We call this variable \(y\) the target variable.
 \(y_\text{obs}\): the \(n_1\) observed data in \(y\) \(y_\text{mis}\): the \(n_0\) missing data in \(y\) \(\dot{y}\): imputed values in \(y\)  Suppose \(X\) are the variables (covariates) in the imputation model.</description>
    </item>
    
    <item>
      <title>Book Notes: Flexible Imputation of Missing Data -- Ch2 Multiple Imputation</title>
      <link>http://liyingbo.com/stat/2020/08/23/book-notes-flexible-imputation-of-missing-data-ch2-multiple-imputation/</link>
      <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/08/23/book-notes-flexible-imputation-of-missing-data-ch2-multiple-imputation/</guid>
      <description>Concepts in Incomplete Data Why and When Multiple Imputation Works More about Imputation Methods   For the pdf slides, click here
Concepts in Incomplete Data Notations  \(m\): number of multiple imputations \(Y\): data of the sample  Includes both covariates and response Dimension \(n \times p\)  \(R\): observation indicator matrix, known  A \(n \times p\) 0-1 matrix \(r_{ij} =0\) for missing and 1 for observed  \(Y_{\text{obs}}\): observed data \(Y_{\text{mis}}\): missing data \(Y = (Y_{\text{obs}}, Y_{\text{mis}})\): complete data</description>
    </item>
    
    <item>
      <title>Book Notes: Flexible Imputation of Missing Data -- Ch1 Introduction</title>
      <link>http://liyingbo.com/stat/2020/08/22/book-notes-flexible-imputation-of-missing-data-ch1-introduction/</link>
      <pubDate>Sat, 22 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/08/22/book-notes-flexible-imputation-of-missing-data-ch1-introduction/</guid>
      <description>Concepts of MCAR, MAR, MNAR Ad-hoc Solutions Multiple Imputation in a Nutshell   For the pdf slides, click here
Concepts of MCAR, MAR, MNAR Concepts of MCAR, MAR, MNAR  Missing completely at random (MCAR): the probability of being missing is the same for all cases  Cause of missing is unrelated to the data  Missing at random (MAR): the probability of being missing only depends on the observed data  Cause of missing is unrelated to the missing values  Missing not at random (MNAR): probability of being missing depends on the missing values themselves    Ad-hoc Solutions Listwise deletion and pairwise deletion  Listwise deletion (also called complete-case analysis): delete rows which contain one or more missing values  If data is MCAR, listwise deletion produces unbiased estimates of means, variances, and regression weights (if need to train a predictive model) If data is not MCAR, listwise deletion can severely bias the above estimates.</description>
    </item>
    
    <item>
      <title>Book Notes: Computer Age Statistical Inference -- Ch9 Survival Analysis</title>
      <link>http://liyingbo.com/stat/2020/06/13/book-notes-computer-age-statistical-inference-ch-9-survival-analysis/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/06/13/book-notes-computer-age-statistical-inference-ch-9-survival-analysis/</guid>
      <description>Survival Analysis Life Table and Kaplan-Meier Estimate Cox’s Proportional Hazards Model    For the pdf slides, click here
Survival Analysis Life Table and Kaplan-Meier Estimate Life table  An insurance company’s life table shows information of clients by their age. For each age \(i\), it contains
 \(n_i\): number of clients \(y_i\): number of death \(\hat{h}_i = y_i / n_i\): hazard rate \(\hat{S}_i\): survival probability estimate  An example life table</description>
    </item>
    
    <item>
      <title>Book Notes: Pattern Recognition and Machine Learning -- Ch9 Mixture Models and EM Algorithm</title>
      <link>http://liyingbo.com/stat/2020/06/13/book-notes-pattern-recognition-and-machine-learning-ch9-mixture-models-and-em-algorithm/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/06/13/book-notes-pattern-recognition-and-machine-learning-ch9-mixture-models-and-em-algorithm/</guid>
      <description>K-means Clustering vs Mixtures of Gaussians K-means clustering Mixture of Gaussians  EM Algorithm The general EM algorithm A different view of the EM algorithm, related to variational inference    For the pdf slides, click here
K-means Clustering vs Mixtures of Gaussians K-means clustering K-means clustering: problem  Data
 \(D\)-dimensional observations: \(\mathbf{x}_1, \ldots, \mathbf{x}_N\)  Parameters
 \(K\) clusters’ means: \(\boldsymbol\mu_1, \ldots, \boldsymbol\mu_K\) Binary indicator \(r_{nk} \in \{0, 1\}\): if object \(n\) is in class \(k\)  Goal: find values for \(\{\boldsymbol\mu_k\}\) and \(\{r_{nk}\}\) to minimize the objective function (called a distortion measure) \[ J = \sum_{n=1}^N \sum_{k = 1}^K r_{nk} \|\mathbf{x}_n - \boldsymbol\mu_k \|^2 \]</description>
    </item>
    
    <item>
      <title>Book Notes: Intro to Time Series and Forecasting -- Ch6 ARIMA Models</title>
      <link>http://liyingbo.com/stat/2020/03/21/book-notes-intro-to-time-series-and-forecasting-ch6-arima-models/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/03/21/book-notes-intro-to-time-series-and-forecasting-ch6-arima-models/</guid>
      <description>ARIMA Models Transformation and Identification Techniques Unit Root Test Forecast ARIMA models  Seasonal ARIMA Models Regression with ARMA Errors   For the pdf slides, click here
When data is not stationary  Implication of not stationary: sample ACF or sample PACF do not rapidly decrease to zero as lag increases
 What shall we do?  Differencing, then fit an ARMA \(\rightarrow\) ARIMA Transformation, then fit an ARMA Seasonal model \(\rightarrow\) SARIMA    A non-stationary exmaple: Dow Jones utilities index data library(itsmr); ## Load the ITSM-R package par(mfrow = c(1, 3)); plot.</description>
    </item>
    
    <item>
      <title>Book Notes: Intro to Time Series and Forecasting -- Ch5 ARMA Models Estimation and Forecasting</title>
      <link>http://liyingbo.com/stat/2020/03/20/book-notes-intro-to-time-series-and-forecasting-ch5-modeling-and-forecasting-with-arma-processes/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/03/20/book-notes-intro-to-time-series-and-forecasting-ch5-modeling-and-forecasting-with-arma-processes/</guid>
      <description>Yule-Walker Estimation Maximum Likelihood Estimation Order Selection Diagnostic Checking   For the pdf slides, click here
Parameter estimation for ARMA\((p, q)\)  When the orders \(p, q\) are known, estimate the parameters \[ \boldsymbol\phi = (\phi_1, \ldots, \phi_p), \quad \boldsymbol\theta = (\theta_1, \ldots, \theta_q), \quad \sigma^2 \]  There are \(p+q+1\) parameters in total  Preliminary estimations  Yule-Walker and Burg’s algorithm: good for AR\((p)\) Innovation algorithm: good for MA\((q)\) Hannan-Rissanen algorithm: good for ARMA\((p, q)\)  More efficient estimation: MLE</description>
    </item>
    
    <item>
      <title>My Old Notes on Python</title>
      <link>http://liyingbo.com/stat/2019/02/22/my-old-notes-on-python/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2019/02/22/my-old-notes-on-python/</guid>
      <description> Part 1: Python Basics Part 2: Numpy Part 3: Pandas Part 4: Advanced programming  </description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch3 ARMA Models</title>
      <link>http://liyingbo.com/stat/2019/01/26/book-notes-introduction-to-time-series-and-forecasting-ch3-arma-models/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2019/01/26/book-notes-introduction-to-time-series-and-forecasting-ch3-arma-models/</guid>
      <description>ARMA\((p, q)\) Processes Stationary solution Causality Invertibility  ACF and PACF of an ARMA\((p, q)\) Process Calculation of the ACVF Test for MAs and ARs from the ACF and PACF  Forecast ARMA Processes   For the pdf slides, click here
ARMA\((p, q)\) Processes ARMA\((p, q)\) process: definitions  \(\{X_t\}\) is an ARMA\((p, q)\) process if it is stationary, and for all \(t\), \[ X_t - \phi_1 X_{t-1} - \cdots - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + \cdots + \theta_q Z_{t-q} \] where \(\{Z_t\} \sim \textrm{WN}(0, \sigma^2)\) and the polynomials \[ \phi(z) = 1 - \phi_1 z - \cdots - \phi_p z^p, \quad \theta(z) = 1 + \theta_1 z + \cdots + \theta_q z^q \] have no common factors</description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch2 Stationary Processes</title>
      <link>http://liyingbo.com/stat/2019/01/19/book-notes-introduction-to-time-series-and-forecasting-ch2-stationary-processes/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2019/01/19/book-notes-introduction-to-time-series-and-forecasting-ch2-stationary-processes/</guid>
      <description>Linear Processes Introduction to ARMA Processes ARMA\((1,1)\) process  Properties of the Sample ACVF and Sample ACF Bartlett’s Formula  Forecast Stationary Time Series Best linear predictor: minimizes MSE Recursive methods: the Durbin-Levinson and Innovation Algorithms    For the pdf slides, click here
Best linear predictor  Goal: find a function of \(X_n\) that gives the “best” predictor of \(X_{n+h}\).
 We mean “best” by achieving minimum mean squared error Under joint normality assumption of \(X_n\) and \(X_{n+h}\), the best estimator is \[ m(X_n) = E(X_{n+h} \mid X_n) = \mu + \rho(h)(X_n - \mu) \]  Best linear predictor \[ \ell(X_n) = a X_n + b \]</description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch1 Introduction</title>
      <link>http://liyingbo.com/stat/2018/12/18/book-notes-introduction-to-time-series-and-forecasting-ch1/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2018/12/18/book-notes-introduction-to-time-series-and-forecasting-ch1/</guid>
      <description>Stationary Models and Autocorrelation Function Examples of Simple Time Series Models  Estimate and Eliminate Trend and Seasonal Components Trend Component Only Also with the Seasonal Component  Test Whether Estimated Noises are IID   For the pdf slides, click here
Objective of time series models  Seasonal adjustment: recognize seasonal components and remove them to study long-term trends
 Separate (or filter) noise from signals
 Prediction</description>
    </item>
    
    <item>
      <title>My first post about statistics!</title>
      <link>http://liyingbo.com/stat/2018/04/03/my-first-post-about-statistics/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2018/04/03/my-first-post-about-statistics/</guid>
      <description>Finally, a post not about cooking &amp;hellip;
I recently gave a 1.5-hours lecture to friends who were interested in Bayesian statistics, and the slides are here.
This lecture covers (very briefly) the following topics
  Bayes formula (i.e., conditional probablity)
 Example: breast cancer screening    Bayesian parameter estimation (i.e., posterior distribution formula)
 Beta-Binomial updating, with a coin flipping example Example: Bayesian inference on the Rotten Tomato &amp;ldquo;freshness&amp;rdquo; of the Harry Potter movie series (This is what the above picture is about) MCMC (very high-level intro)    Bayesian hypothesis testing</description>
    </item>
    
  </channel>
</rss>
