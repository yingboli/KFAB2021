<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>King Fox And Butterfly</title>
<meta name="description" content="A blog about cooking and statistics.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="http://liyingbo.com/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="http://liyingbo.com/css/font-awesome.min.css">
<link rel="stylesheet" href="http://liyingbo.com/css/owl.carousel.css">
<link rel="stylesheet" href="http://liyingbo.com/css/owl.theme.css">


  <link href="http://liyingbo.com/css/style.blue.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="http://liyingbo.com/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="http://liyingbo.com/img/favicon.png">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-116913878-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="http://liyingbo.com">King Fox And Butterfly</a></h1>
    
      <p class="sidebar-p">I am a home cook and statistian, both with more than 10 years of experience.</p>
    
      <p class="sidebar-p">Originally from Shanghai, currently based in Dallas.</p>
    
    <ul class="sidebar-menu">
      
        <li><a href="http://liyingbo.com/cooking/">Cooking</a></li>
      
        <li><a href="http://liyingbo.com/stat/">Statistics</a></li>
      
        <li><a href="http://liyingbo.com/about/">About Me</a></li>
      
        <li><a href="http://liyingbo.com/categories/">Categories</a></li>
      
        <li><a href="http://liyingbo.com/tags/">Tags</a></li>
      
    </ul>
    <p class="social">
  
  
  
  
  
  
  <a href="https://www.linkedin.com/in/yingbo-li-08321723/" data-animate-hover="pulse" class="external">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/yingboli" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy; Yingbo Li 2018 -- 2021 |
        
        Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="http://liyingbo.com">King Fox And Butterfly</a></h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Book Notes: Pattern Recognition and Machine Learning -- Ch9 Mixture Models and EM Algorithm</h1>
         

<div id="TOC">
<ul>
<li><a href="#k-means-clustering-vs-mixtures-of-gaussians">K-means Clustering vs Mixtures of Gaussians</a><ul>
<li><a href="#k-means-clustering">K-means clustering</a></li>
<li><a href="#mixture-of-gaussians">Mixture of Gaussians</a></li>
</ul></li>
<li><a href="#em-algorithm">EM Algorithm</a><ul>
<li><a href="#the-general-em-algorithm">The general EM algorithm</a></li>
<li><a href="#a-different-view-of-the-em-algorithm-related-to-variational-inference">A different view of the EM algorithm, related to variational inference</a></li>
</ul></li>
</ul>
</div>

<p><strong><em>For the pdf slides, click <a href="/pdf/050120_EM_algorithm_ch9.pdf">here</a></em></strong></p>
<div id="k-means-clustering-vs-mixtures-of-gaussians" class="section level1">
<h1>K-means Clustering vs Mixtures of Gaussians</h1>
<div id="k-means-clustering" class="section level2">
<h2>K-means clustering</h2>
<div id="k-means-clustering-problem" class="section level3">
<h3>K-means clustering: problem</h3>
<ul>
<li><p>Data</p>
<ul>
<li><span class="math inline">\(D\)</span>-dimensional observations: <span class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_N\)</span></li>
</ul></li>
<li><p>Parameters</p>
<ul>
<li><span class="math inline">\(K\)</span> clustersâ€™ means: <span class="math inline">\(\boldsymbol\mu_1, \ldots, \boldsymbol\mu_K\)</span></li>
<li>Binary indicator <span class="math inline">\(r_{nk} \in \{0, 1\}\)</span>: if object <span class="math inline">\(n\)</span> is in class <span class="math inline">\(k\)</span></li>
</ul></li>
<li><p>Goal: find values for <span class="math inline">\(\{\boldsymbol\mu_k\}\)</span> and <span class="math inline">\(\{r_{nk}\}\)</span>
to minimize the objective function (called a <font color='blue'>distortion measure</font>)
<span class="math display">\[
J = \sum_{n=1}^N \sum_{k = 1}^K r_{nk} \|\mathbf{x}_n - \boldsymbol\mu_k \|^2
\]</span></p></li>
</ul>
</div>
<div id="k-means-clustering-solution" class="section level3">
<h3>K-means clustering: solution</h3>
<ul>
<li><p>Two-stage optimization</p>
<ul>
<li>Update <span class="math inline">\(r_{nk}\)</span> and <span class="math inline">\(\boldsymbol\mu_k\)</span> alternatively, and repeat until convergence</li>
<li>Resembles the E step and M step in the EM algorithm</li>
</ul></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>E(expectation) step: updates <span class="math inline">\(r_{nk}\)</span>.</p>
<ul>
<li>Assign the <span class="math inline">\(n\)</span>th data point to the closest cluster center
<span class="math display">\[
 r_{nk} = \begin{cases}
 1 &amp; \text{ if } k = \arg\min_j \|\mathbf{x}_n - \boldsymbol\mu_k \|^2\\
 0 &amp; \text{ otherwise}
 \end{cases}
 \]</span></li>
</ul></li>
<li><p>M(maximization) step: updates <span class="math inline">\(\boldsymbol\mu_k\)</span></p>
<ul>
<li>Set cluster mean to be mean of all data points assigned to this cluster
<span class="math display">\[
 \boldsymbol\mu_k = \frac{\sum_{n} r_{nk} \mathbf{x}_n}{\sum_{n} r_{nk}}
 \]</span></li>
</ul></li>
</ol>
</div>
</div>
<div id="mixture-of-gaussians" class="section level2">
<h2>Mixture of Gaussians</h2>
<div id="mixture-of-gaussians-definition" class="section level3">
<h3>Mixture of Gaussians: definition</h3>
<ul>
<li><p><font color='blue'>Mixture of Gaussians</font>: log likelihood
<span class="math display">\[\begin{equation}\label{eq:mixture_of_gaussians}
\log p(\mathbf{x}) = 
\log \left\{
\sum_{k = 1}^K \pi_k \cdot \text{N}\left(\mathbf{x} \mid \boldsymbol\mu_k, \boldsymbol\Sigma_k \right)
\right\}
\end{equation}\]</span></p></li>
<li><p>Introduce a <span class="math inline">\(K\)</span>-dim latent indicator variable <span class="math inline">\(\mathbf{z}\in \{0, 1\}^K\)</span>
<span class="math display">\[
z_k = \mathbf{1}(\text{if $\mathbf{x}$ is from the $k$-th Gaussian component})
\]</span></p></li>
</ul>
<p>The marginal distribution of <span class="math inline">\(\mathbf{z}\)</span> is multinomial
<span class="math display">\[
p(z_k = 1) = \pi_k
\]</span></p>
<ul>
<li>We call the posterior probability as the <font color='blue'>Responsibility</font> that component <span class="math inline">\(k\)</span> takes for
explaining the observation <span class="math inline">\(\mathbf{x}\)</span>
<span class="math display">\[
\gamma(z_k) = p(z_k = 1\mid \mathbf{x}) = 
\frac{\pi_k \cdot \text{N}\left(\mathbf{x} \mid \boldsymbol\mu_k, \boldsymbol\Sigma_k \right)}
{\sum_{j=1}^K \pi_j \cdot \text{N}\left(\mathbf{x} \mid \boldsymbol\mu_j, \boldsymbol\Sigma_j \right)}
\]</span></li>
</ul>
</div>
<div id="mixture-of-gaussians-singularity-problem-with-mle" class="section level3">
<h3>Mixture of Gaussians: singularity problem with MLE</h3>
<ul>
<li><p><font color='red'>Problem with maximum likelihood estimation: presence of singularities</font>:
there will be clusters that contains only one data point,
so that the corresponding covariance matrix will be estimated at zero, thus the likelihood explodes</p>
<ul>
<li><p><font color='green'>Therefore, when finding MLE, we should avoid finding such singularity solution and
instead seek well-behaved local maxima of the likelihood function</font>: see the following EM approach</p></li>
<li><p>Alternatively, we can to adopt a Bayesian approach</p></li>
</ul></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="/figures/Bishop_fig_9_7.png" alt="Illustration of singularities" width="35%" />
<p class="caption">
Figure 1: Illustration of singularities
</p>
</div>
</div>
<div id="conditional-mle-of-boldsymbolmu_k" class="section level3">
<h3>Conditional MLE of <span class="math inline">\(\boldsymbol\mu_k\)</span></h3>
<ul>
<li>Suppose we observe <span class="math inline">\(N\)</span> data points <span class="math inline">\(\mathbf{X} = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\}\)</span></li>
<li><p>Similarly, we write the <span class="math inline">\(N\)</span> latent variables as <span class="math inline">\(\mathbf{Z} = \{\mathbf{z}_1, \ldots, \mathbf{z}_N\}\)</span></p></li>
<li><p>Set the derivatives of <span class="math inline">\(\log p(\mathbf{X} \mid \boldsymbol\pi, \boldsymbol\mu, \boldsymbol\Sigma)\)</span>
with respect to <span class="math inline">\(\boldsymbol\mu\)</span> to zero
<span class="math display">\[
0 = \sum_{n=1}^N \gamma(z_{nk}) ~\boldsymbol\Sigma_k ~\left(\mathbf{x}_n - \boldsymbol\mu_k \right)
\]</span>
Then we obtain
<span class="math display">\[
\boldsymbol\mu_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})~ \mathbf{x}_n
\]</span>
where <span class="math inline">\(N_k\)</span> is the effective number of points assigned to cluster <span class="math inline">\(k\)</span>
<span class="math display">\[
N_k = \sum_{n=1}^N \gamma(z_{nk})
\]</span></p></li>
</ul>
</div>
<div id="conditional-mle-of-boldsymbolsigma_k-and-pi_k" class="section level3">
<h3>Conditional MLE of <span class="math inline">\(\boldsymbol\Sigma_k\)</span> and <span class="math inline">\(\pi_k\)</span></h3>
<ul>
<li><p>Similarly, setting the derivatives of log likelihood wrt <span class="math inline">\(\boldsymbol\Sigma_k\)</span>, we have
<span class="math display">\[
\boldsymbol\Sigma_k = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})~ 
\left(\mathbf{x}_n - \boldsymbol\mu_k\right)\left(\mathbf{x}_n - \boldsymbol\mu_k\right)^\top
\]</span></p></li>
<li><p>Use Lagrange multiplier to maximize log likelihood wrt <span class="math inline">\(\pi_k\)</span> under the constraint
that all <span class="math inline">\(\pi_k\)</span> add up to one:
<span class="math display">\[
\log p(\mathbf{X} \mid \boldsymbol\pi, \boldsymbol\mu, \boldsymbol\Sigma) + 
\lambda \left( \sum_{k=1}^K \pi_k - 1 \right)
\]</span>
we get the solution
<span class="math display">\[
\pi_k = \frac{N_k}{N}
\]</span></p></li>
<li><p><font color='red'>The above results on </font><span class="math inline">\(\boldsymbol\mu_k, \boldsymbol\Sigma_k, \pi_k\)</span> <font color='red'>are not
closed-form solution because the responsibilities</font> <span class="math inline">\(\gamma(z_{nk})\)</span> <font color='red'>depend on them
in a complex way.</font></p></li>
</ul>
</div>
<div id="em-algorithm-for-mixture-of-gaussians" class="section level3">
<h3>EM algorithm for mixture of Gaussians</h3>
<ol style="list-style-type: decimal">
<li><p>Initialize <span class="math inline">\(\boldsymbol\mu_k, \boldsymbol\Sigma_k, \pi_k\)</span>, usually using the <span class="math inline">\(K\)</span>-means algorithm.</p></li>
<li><p><strong><font color='green'>E step</font></strong>: compute responsibilities using the current parameters
<span class="math display">\[
\gamma(z_{nk}) = 
\frac{\pi_k \cdot \text{N}\left(\mathbf{x}_n \mid \boldsymbol\mu_k, \boldsymbol\Sigma_k \right)}
{\sum_{j=1}^K \pi_j \cdot \text{N}\left(\mathbf{x}_n \mid \boldsymbol\mu_j, \boldsymbol\Sigma_j \right)}
\]</span></p></li>
<li><p><strong><font color='green'>M step</font></strong>: re-estimate the parameters using the current responsibilities,
where <span class="math inline">\(N_k = \sum_{n=1}^N \gamma(z_{nk})\)</span>
<span class="math display">\[\begin{align*}
\boldsymbol\mu_k^{\text{new}} &amp; = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})~ \mathbf{x}_n\\
\boldsymbol\Sigma_k^{\text{new}} &amp; = \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk})~ 
\left(\mathbf{x}_n - \boldsymbol\mu_k\right)\left(\mathbf{x}_n - \boldsymbol\mu_k\right)^\top\\
\pi_k^{\text{new}} &amp; = \frac{N_k}{N}
\end{align*}\]</span></p></li>
<li><p>Check for convergence of either the parameters or the log likelihood. If not converged, return to step 2.</p></li>
</ol>
</div>
<div id="connection-between-k-means-and-gaussian-mixture-model" class="section level3">
<h3>Connection between K-means and Gaussian mixture model</h3>
<ul>
<li><p>K-means algorithm itself is often used to initialize the parameters in
a Gaussian mixture model before applying the EM algorithm</p></li>
<li><p>Mixture of Gaussians: soft assignment of data points to clusters,
using posterior probabilities</p></li>
<li><p><span class="math inline">\(K\)</span>-means can be viewed as a special case of mixture of Gaussian,
where covariances of mixture components are <span class="math inline">\(\epsilon \mathbf{I}\)</span>,
where <span class="math inline">\(\epsilon\)</span> is a parameter shared by all components.</p>
<ul>
<li>In the responsibility calculation,
<span class="math display">\[
  \gamma(z_{nk}) = \frac{\pi_k \exp\{-\| \mathbf{x}_n - \boldsymbol\mu_k\|^2 / 2\epsilon \}}
  {\sum_j \pi_j \exp\{-\| \mathbf{x}_n - \boldsymbol\mu_j\|^2 / 2\epsilon \}}
  \]</span>
In the limit <span class="math inline">\(\epsilon\rightarrow 0\)</span>, for each observation <span class="math inline">\(n\)</span>,
the responsibilities <span class="math inline">\(\{\gamma(z_{nk}), k = 1, \ldots, K\}\)</span> has exactly one unity and all the rest are zero.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="em-algorithm" class="section level1">
<h1>EM Algorithm</h1>
<div id="the-general-em-algorithm" class="section level2">
<h2>The general EM algorithm</h2>
<div id="em-algorithm-definition" class="section level3">
<h3>EM algorithm: definition</h3>
<ul>
<li><p>Goal: maximize likelihood <span class="math inline">\(p(\mathbf{X} \mid \boldsymbol\theta)\)</span> with respect to the parameter <span class="math inline">\(\boldsymbol\theta\)</span>,
for models having latent variables <span class="math inline">\(\mathbf{Z}\)</span>.</p></li>
<li>Notations
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span>: observed data; also called <font color='blue'>incomplete</font> data</li>
<li><span class="math inline">\(\boldsymbol\theta\)</span>: model parameters</li>
<li><span class="math inline">\(\mathbf{Z}\)</span>: latent variables, usually each observation has a latent variable</li>
<li><span class="math inline">\(\{\mathbf{X}, \mathbf{Z}\}\)</span> is called <font color='blue'>complete data</font></li>
</ul></li>
<li>Log likelihood
<span class="math display">\[
\log~p(\mathbf{X} \mid \boldsymbol\theta) = 
\log\left\{ \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta) \right\}
\]</span>
<ul>
<li>The sum over <span class="math inline">\(\mathbf{Z}\)</span> can be replaced by an integral if <span class="math inline">\(\mathbf{Z}\)</span> is continuous</li>
<li><font color='red'>The presence of sum prevents the logarithm from acting directly on the joint distribution.
This complicates MLE solutions, especially for exponential family.</font></li>
</ul></li>
</ul>
</div>
<div id="general-em-algorithm-two-stage-iterative-optimization" class="section level3">
<h3><font color='blue'>General EM algorithm</font>: two-stage iterative optimization</h3>
<ol style="list-style-type: decimal">
<li><p>Choose the initial parameters <span class="math inline">\(\boldsymbol\theta^{\text{old}}\)</span></p></li>
<li><p><strong>E step</strong>: since the conditional posterior
<span class="math inline">\(p\left( \mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}} \right)\)</span>
contains all of our knowledge about the latent variable <span class="math inline">\(\mathbf{Z}\)</span>,
we compute the expected complete-data log likelihood under it.
<span class="math display">\[\begin{align*}
\mathcal{Q}(\boldsymbol\theta, \boldsymbol\theta^{\text{old}})
&amp; = E_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}}}
\left\{\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)  \right\} \\
&amp; = \sum_{\mathbf{Z}} p\left( \mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}} \right)
\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)
\end{align*}\]</span></p></li>
<li><strong>M step</strong>: revise parameter estimate
<span class="math display">\[
\boldsymbol\theta^{\text{new}} = \arg\max_{\boldsymbol\theta} 
\mathcal{Q}(\boldsymbol\theta, \boldsymbol\theta^{\text{old}})
\]</span>
<ul>
<li><font color='green'>Note in the maximizing step, the logarithm acts driectly on the joint likelihood</font> <span class="math inline">\(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)\)</span>, <font color='green'>so the maximizating will be tractable.</font></li>
</ul></li>
<li><p>Check for convergence of the log likelihood or the parameter values.
If not converged, use <span class="math inline">\(\boldsymbol\theta^{\text{new}}\)</span> to replace <span class="math inline">\(\boldsymbol\theta^{\text{old}}\)</span>,
and return to step 2.</p></li>
</ol>
</div>
<div id="gaussian-mixtures-revisited" class="section level3">
<h3>Gaussian mixtures revisited</h3>
<ul>
<li>Recall that latent variables <span class="math inline">\(\mathbf{Z}\in \mathbb{R}^{N \times K}\)</span> :
<span class="math display">\[
z_{nk} = \mathbf{1}(\text{if $\mathbf{x}_n$ is from the $k$-th Gaussian component})
\]</span></li>
<li>Complete data log likelihood
<span class="math display">\[
\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\mu, \boldsymbol\Sigma, \boldsymbol\pi)
= \sum_{n=1}^N \sum_{k=1}^K z_{nk}\left\{\log \pi_k + 
\log \text{N}\left(\mathbf{x}_n \mid  \boldsymbol\mu_k, \boldsymbol\Sigma_k \right) \right\}
\]</span>
<ul>
<li>Comparing this with incomplete data log likelihood in Eq ,
<font color='green'>we have the sum over <span class="math inline">\(k\)</span> and logarithm interchanged. Thus,
the logarithm acts on Gaussian density directly.</font></li>
</ul></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="/figures/Bishop_fig_9_9.png" alt="Mixture of Gaussians, treating latent variables as observed" width="30%" />
<p class="caption">
Figure 2: Mixture of Gaussians, treating latent variables as observed
</p>
</div>
</div>
<div id="continue-gaussian-mixtures-revisited" class="section level3">
<h3>Continue: Gaussian mixtures revisited</h3>
<ul>
<li><p>Conditional posterior of <span class="math inline">\(\mathbf{Z}\)</span>
<span class="math display">\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\mu, \boldsymbol\Sigma, \boldsymbol\pi)
\propto \prod_{n=1}^N \prod_{k=1}^K \left[\pi_k \text{N}\left(\mathbf{x}_n \mid  \boldsymbol\mu_k, \boldsymbol\Sigma_k \right) \right]^{z_{nk}}
\]</span>
Thus, the conditional posterior of <span class="math inline">\(\{\mathbf{z}_n\}\)</span> are independent</p></li>
<li><p>Conditional expectations
<span class="math display">\[
E_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol\mu^{\text{old}}, 
\boldsymbol\Sigma^{\text{old}}, \boldsymbol\pi^{\text{old}}}~z_{nk} = \gamma(z_{nk})^{\text{old}}
\]</span></p></li>
<li><p>Thus the objective function in the M-step
<span class="math display">\[\begin{align*}
&amp; E_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol\mu^{\text{old}}, 
\boldsymbol\Sigma^{\text{old}}, \boldsymbol\pi^{\text{old}}}~
\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\mu, \boldsymbol\Sigma, \boldsymbol\pi)\\
= &amp; \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk})^{\text{old}}\left\{\log \pi_k + 
\log \text{N}\left(\mathbf{x}_n \mid  \boldsymbol\mu_k, \boldsymbol\Sigma_k \right) \right\}
\end{align*}\]</span></p></li>
</ul>
</div>
</div>
<div id="a-different-view-of-the-em-algorithm-related-to-variational-inference" class="section level2">
<h2>A different view of the EM algorithm, related to variational inference</h2>
<div id="a-different-view-of-the-em-algorithm" class="section level3">
<h3>A different view of the EM algorithm</h3>
<ul>
<li><p>Goal: maximize the incomplete data likelihood
<span class="math display">\[
p(\mathbf{X} \mid \boldsymbol\theta) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)
\]</span></p></li>
<li><p>Suppose that optimization of <span class="math inline">\(p(\mathbf{X} \mid \boldsymbol\theta)\)</span> is difficult,
but optimization of <span class="math inline">\(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)\)</span> is significantly easier.</p></li>
<li><p><font color='red'>An important decompsition</font>: holds for any arbitrary distribution <span class="math inline">\(q(\mathbf{Z})\)</span>
<span class="math display">\[\begin{equation}\label{eq:em_decomposition}
\log p(\mathbf{X} \mid \boldsymbol\theta) = \mathcal{L}(q, \boldsymbol\theta) + \text{KL}(q ~\| ~p)
\end{equation}\]</span>
where <span class="math inline">\(\mathcal{L}(q, \boldsymbol\theta)\)</span> is called a <font color='blue'>lower bound</font> on <span class="math inline">\(\log p(\mathbf{X} \mid \boldsymbol\theta)\)</span>:
<span class="math display">\[\begin{align*}
\mathcal{L}(q, \boldsymbol\theta)
&amp; = \sum_{\mathbf{Z}} q(\mathbf{Z}) 
\log\left\{ \frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)}{q(\mathbf{Z})} \right\}\\
\text{KL}(q ~\| ~p)
&amp; = - \sum_{\mathbf{Z}} q(\mathbf{Z})
\log\left\{ \frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta)}{q(\mathbf{Z})} \right\}
\end{align*}\]</span></p>
<ul>
<li><font color='green'>Note: this formula will appear again in variational inference.</font></li>
</ul></li>
</ul>
</div>
<div id="a-different-view-of-the-em-algorithm-e-step" class="section level3">
<h3>A different view of the EM algorithm: E step</h3>
<ul>
<li><p>In E step, the lower bound <span class="math inline">\(\mathcal{L}(q, \boldsymbol\theta^{\text{old}})\)</span> is maximized
with respect to <span class="math inline">\(q(\mathbf{Z})\)</span> while keeping <span class="math inline">\(\boldsymbol\theta^{\text{old}}\)</span> fixed</p></li>
<li><p>The solution is when the KL divergence <span class="math inline">\(\text{KL}\left(q(\mathbf{Z}) ~\|~ p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}}\right) \right)\)</span> is zero, i.e.,
<span class="math display">\[
q(\mathbf{Z}) = p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}}\right)
\]</span></p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="/figures/Bishop_fig_9_12.png" alt="In the E step, the lower bound moves to the same value as the old incomplete data log likelihood" width="55%" />
<p class="caption">
Figure 3: In the E step, the lower bound moves to the same value as the old incomplete data log likelihood
</p>
</div>
</div>
<div id="a-different-view-of-the-em-algorithm-m-step" class="section level3">
<h3>A different view of the EM algorithm: M step</h3>
<ul>
<li><p>In M step, the distribution <span class="math inline">\(q(\mathbf{Z})\)</span> is held fixed and the lower bound
<span class="math inline">\(\mathcal{L}(q, \boldsymbol\theta^{\text{old}})\)</span> is maximized wrt <span class="math inline">\(\boldsymbol\theta\)</span>
to give some new value <span class="math inline">\(\boldsymbol\theta^{\text{new}}\)</span>. Thus, the lower bound increases.</p></li>
<li><p>Since <span class="math inline">\(q(\mathbf{Z})\)</span> is fixed at <span class="math inline">\(\boldsymbol\theta^{\text{old}}\)</span>, it will not equal
the new posterior <span class="math inline">\(p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{new}}\right)\)</span>.
Therefore, the KL divergence becomes nonzero.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="/figures/Bishop_fig_9_13.png" alt="In the M step, both the lower bound and the KL divergence increase." width="45%" />
<p class="caption">
Figure 4: In the M step, both the lower bound and the KL divergence increase.
</p>
</div>
</div>
<div id="em-algorithm-illustration" class="section level3">
<h3>EM algorithm illustration</h3>
<ul>
<li>Red curve: incomplete data log likelihood</li>
<li>Blue curve: lower bound <span class="math inline">\(\mathcal{L}(\boldsymbol\theta, \boldsymbol\theta^{\text{old}})\)</span></li>
<li>Green curve: lower bound <span class="math inline">\(\mathcal{L}(\boldsymbol\theta, \boldsymbol\theta^{\text{new}})\)</span></li>
<li>The lower bounds have tangential contacts with the log likelihood</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="/figures/Bishop_fig_9_14.png" alt="Illustration of EM algorithm, in the parameter space" width="65%" />
<p class="caption">
Figure 5: Illustration of EM algorithm, in the parameter space
</p>
</div>
</div>
<div id="em-algorithm-in-bayesian-statistics" class="section level3">
<h3>EM algorithm in Bayesian statistics</h3>
<ul>
<li><p>EM algorithm can be used to estimate maximum posterior (MAP)</p></li>
<li><p>In this case, the objective function is
<span class="math display">\[
p(\boldsymbol\theta \mid \mathbf{X}) \propto  p(\mathbf{X} \mid \boldsymbol\theta) ~ p(\boldsymbol\theta)
\]</span>
Hence, the expectation in Step 2 becomes
<span class="math display">\[\begin{align*}
\mathcal{Q}(\boldsymbol\theta, \boldsymbol\theta^{\text{old}})
&amp; = E_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}}}
 \left\{\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta) + \log p(\boldsymbol\theta) \right\} \\
&amp; = E_{\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta^{\text{old}}}
 \left\{\log p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol\theta)\right\}  + \log p(\boldsymbol\theta) 
\end{align*}\]</span></p></li>
</ul>
</div>
<div id="em-algorithm-and-missing-data" class="section level3">
<h3>EM algorithm and missing data</h3>
<ul>
<li>The latent variables can be the missing values in the data</li>
<li>This is valid is the data are <strong>missing at random</strong></li>
</ul>
</div>
<div id="em-algorithm-for-iid-data-with-n-latent-variables" class="section level3">
<h3>EM algorithm for IID data with <span class="math inline">\(N\)</span> latent variables</h3>
<ul>
<li>Suppose <span class="math inline">\(N\)</span> data points <span class="math inline">\(\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}\)</span> are IID</li>
<li><p>Each observation <span class="math inline">\(\mathbf{x}_n\)</span> has its corresponding latent variable <span class="math inline">\(\mathbf{z}_n\)</span></p></li>
<li><p>Then the conditional posterior of <span class="math inline">\(\mathbf{Z}\)</span> also factorizes wrt <span class="math inline">\(n\)</span>:
<span class="math display">\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol\theta) = 
\prod_{n=1}^N p(\mathbf{z}_n \mid \mathbf{x}_n, \boldsymbol\theta)
\]</span></p></li>
<li>Exploit this structure: using incremental form of EM that at each cycle only process one data point
<ul>
<li>Benefit: no need to wait for the whole data set to finish processing</li>
</ul></li>
</ul>
</div>
<div id="extensions-of-em-algorithms" class="section level3">
<h3>Extensions of EM algorithms</h3>
<ul>
<li><p>For complex models, E step and/or M step can be intractable</p></li>
<li><font color='blue'>Generalized EM (GEM)</font>: address an intractable M step
<ul>
<li>Instead of maximizing the objective function in the M step, just changing the parameter to increase its value</li>
<li>E.g., using nonlinear optimization methods such as conjugate gradients algorithm</li>
<li>E.g., expected conditional maximization (ECM), constrained optimization</li>
</ul></li>
<li><p>We can also generalize the E step: find <span class="math inline">\(q(\mathbf{Z})\)</span> to partially, rather than completely,
optimize <span class="math inline">\(\mathcal{L}(q, \boldsymbol\theta)\)</span></p></li>
</ul>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.</li>
</ul>
</div>
</div>
</div>

         <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "kfab" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="http://liyingbo.com/js/jquery.min.js"></script>
<script src="http://liyingbo.com/js/bootstrap.min.js"></script>
<script src="http://liyingbo.com/js/jquery.cookie.js"> </script>
<script src="http://liyingbo.com/js/ekko-lightbox.js"></script>
<script src="http://liyingbo.com/js/jquery.scrollTo.min.js"></script>
<script src="http://liyingbo.com/js/masonry.pkgd.min.js"></script>
<script src="http://liyingbo.com/js/imagesloaded.pkgd.min.js"></script>
<script src="http://liyingbo.com/js/owl.carousel.min.js"></script>
<script src="http://liyingbo.com/js/front.js"></script>




<script>
    MathJax = {
        tex: {
            macros: {
                E: ['\\mathbf{E}'],
                V: ['\\mathbf{Var}'],
                C: ['\\mathbf{Cov}'],
                AV: ['\\mathbf{AVar}']
            }
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</body>
</html>
