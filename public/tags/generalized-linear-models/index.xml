<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Generalized Linear Models on King Fox And Butterfly</title>
    <link>http://liyingbo.com/tags/generalized-linear-models/</link>
    <description>Recent content in Generalized Linear Models on King Fox And Butterfly</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 21 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="http://liyingbo.com/tags/generalized-linear-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Book Notes: Generalized Additive Models - Ch3 Generalized Linear Models (GLM)</title>
      <link>http://liyingbo.com/stat/2021/03/21/book-notes-generalized-additive-models-ch3-generalized-linear-models-glm/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2021/03/21/book-notes-generalized-additive-models-ch3-generalized-linear-models-glm/</guid>
      <description>Theory of GLMs Exponential family Iteratively re-weighted least square (IRLS) Asymptotic consistency of MLE, deviance, tests, residuals Quasi-likelihood (GEE)  Generalized Linear Mixed Models (GLMM)   For the pdf slides, click here
GLM overview  In a GLM, a smooth monotonic link function \(g(\cdot)\) connects the expectation \(\mu_i = E(Y_i)\) with the linear combination of \(\mathbf{X}_i\), \[\begin{equation}\label{eq:glm_link} g(\mu_i) = \eta_i = \mathbf{X}_i \boldsymbol\beta \end{equation}\]
 In a generalized linear mixed model (GLMM), we have \[ g(\mu_i) = \eta_i = \mathbf{X}_i \boldsymbol\beta + \mathbf{Z}_i \mathbf{b}, \quad \mathbf{b} \sim \text{N}(\mathbf{0}, \boldsymbol\psi) \]</description>
    </item>
    
    <item>
      <title>Paper Notes: Proper Scoring Rules and Cost Weighted Loss Functions for Binary Classification</title>
      <link>http://liyingbo.com/stat/2020/10/12/paper-notes-proper-scoring-rules-and-cost-weighted-loss-functions-for-binary-classification/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/10/12/paper-notes-proper-scoring-rules-and-cost-weighted-loss-functions-for-binary-classification/</guid>
      <description>Proper Scoring Rules Commonly Used Proper Scoring Rules Structure of Proper Scoring Rules Proper scoring rules are mixtures of cost-weighted misclassification losses Beta Family of Proper Scoring Rules Examples   For the pdf slides, click here
Notations: in binary classification  We are interested in fitting a model \(q(\mathbf{x})\) for the true conditional class 1 probability \[ \eta(\mathbf{x}) = P(Y = 1 \mid \mathbf{X} = \mathbf{x}) \]</description>
    </item>
    
    <item>
      <title>Paper Notes: Generalized R Squared</title>
      <link>http://liyingbo.com/stat/2020/10/05/paper-notes-generalized-r-squared/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/10/05/paper-notes-generalized-r-squared/</guid>
      <description>Generalized \(R^2\) by Cox and Snell Generalized \(R^2\) by Nagelkerke Generalized \(R^2\) for binary data    For the pdf slides, click here
\(R^2\) for normal linear regression  \(R^2\), also called coefficient of determination or multiple correlation coefficient, is defined for normal linear regression, as the proportion of variance “explained” by the regression model \[\begin{equation}\label{eq:R2} R^2 = \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \bar{y} \right)^2} \end{equation}\]</description>
    </item>
    
  </channel>
</rss>
