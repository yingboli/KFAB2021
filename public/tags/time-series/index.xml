<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Time Series on King Fox And Butterfly</title>
    <link>http://liyingbo.com/tags/time-series/</link>
    <description>Recent content in Time Series on King Fox And Butterfly</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="http://liyingbo.com/tags/time-series/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Book Notes: Intro to Time Series and Forecasting -- Ch6 ARIMA Models</title>
      <link>http://liyingbo.com/stat/2020/03/21/book-notes-intro-to-time-series-and-forecasting-ch6-arima-models/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/03/21/book-notes-intro-to-time-series-and-forecasting-ch6-arima-models/</guid>
      <description>ARIMA Models Transformation and Identification Techniques Unit Root Test Forecast ARIMA models  Seasonal ARIMA Models Regression with ARMA Errors   For the pdf slides, click here
When data is not stationary  Implication of not stationary: sample ACF or sample PACF do not rapidly decrease to zero as lag increases
 What shall we do?  Differencing, then fit an ARMA \(\rightarrow\) ARIMA Transformation, then fit an ARMA Seasonal model \(\rightarrow\) SARIMA    A non-stationary exmaple: Dow Jones utilities index data library(itsmr); ## Load the ITSM-R package par(mfrow = c(1, 3)); plot.</description>
    </item>
    
    <item>
      <title>Book Notes: Intro to Time Series and Forecasting -- Ch5 ARMA Models Estimation and Forecasting</title>
      <link>http://liyingbo.com/stat/2020/03/20/book-notes-intro-to-time-series-and-forecasting-ch5-modeling-and-forecasting-with-arma-processes/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2020/03/20/book-notes-intro-to-time-series-and-forecasting-ch5-modeling-and-forecasting-with-arma-processes/</guid>
      <description>Yule-Walker Estimation Maximum Likelihood Estimation Order Selection Diagnostic Checking   For the pdf slides, click here
Parameter estimation for ARMA\((p, q)\)  When the orders \(p, q\) are known, estimate the parameters \[ \boldsymbol\phi = (\phi_1, \ldots, \phi_p), \quad \boldsymbol\theta = (\theta_1, \ldots, \theta_q), \quad \sigma^2 \]  There are \(p+q+1\) parameters in total  Preliminary estimations  Yule-Walker and Burg’s algorithm: good for AR\((p)\) Innovation algorithm: good for MA\((q)\) Hannan-Rissanen algorithm: good for ARMA\((p, q)\)  More efficient estimation: MLE</description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch3 ARMA Models</title>
      <link>http://liyingbo.com/stat/2019/01/26/book-notes-introduction-to-time-series-and-forecasting-ch3-arma-models/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2019/01/26/book-notes-introduction-to-time-series-and-forecasting-ch3-arma-models/</guid>
      <description>ARMA\((p, q)\) Processes Stationary solution Causality Invertibility  ACF and PACF of an ARMA\((p, q)\) Process Calculation of the ACVF Test for MAs and ARs from the ACF and PACF  Forecast ARMA Processes   For the pdf slides, click here
ARMA\((p, q)\) Processes ARMA\((p, q)\) process: definitions  \(\{X_t\}\) is an ARMA\((p, q)\) process if it is stationary, and for all \(t\), \[ X_t - \phi_1 X_{t-1} - \cdots - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + \cdots + \theta_q Z_{t-q} \] where \(\{Z_t\} \sim \textrm{WN}(0, \sigma^2)\) and the polynomials \[ \phi(z) = 1 - \phi_1 z - \cdots - \phi_p z^p, \quad \theta(z) = 1 + \theta_1 z + \cdots + \theta_q z^q \] have no common factors</description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch2 Stationary Processes</title>
      <link>http://liyingbo.com/stat/2019/01/19/book-notes-introduction-to-time-series-and-forecasting-ch2-stationary-processes/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2019/01/19/book-notes-introduction-to-time-series-and-forecasting-ch2-stationary-processes/</guid>
      <description>Linear Processes Introduction to ARMA Processes ARMA\((1,1)\) process  Properties of the Sample ACVF and Sample ACF Bartlett’s Formula  Forecast Stationary Time Series Best linear predictor: minimizes MSE Recursive methods: the Durbin-Levinson and Innovation Algorithms    For the pdf slides, click here
Best linear predictor  Goal: find a function of \(X_n\) that gives the “best” predictor of \(X_{n+h}\).
 We mean “best” by achieving minimum mean squared error Under joint normality assumption of \(X_n\) and \(X_{n+h}\), the best estimator is \[ m(X_n) = E(X_{n+h} \mid X_n) = \mu + \rho(h)(X_n - \mu) \]  Best linear predictor \[ \ell(X_n) = a X_n + b \]</description>
    </item>
    
    <item>
      <title>Book Notes: Introduction to Time Series and Forecasting --  Ch1 Introduction</title>
      <link>http://liyingbo.com/stat/2018/12/18/book-notes-introduction-to-time-series-and-forecasting-ch1/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://liyingbo.com/stat/2018/12/18/book-notes-introduction-to-time-series-and-forecasting-ch1/</guid>
      <description>Stationary Models and Autocorrelation Function Examples of Simple Time Series Models  Estimate and Eliminate Trend and Seasonal Components Trend Component Only Also with the Seasonal Component  Test Whether Estimated Noises are IID   For the pdf slides, click here
Objective of time series models  Seasonal adjustment: recognize seasonal components and remove them to study long-term trends
 Separate (or filter) noise from signals
 Prediction</description>
    </item>
    
  </channel>
</rss>
