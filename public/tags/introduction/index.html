<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>King Fox And Butterfly</title>
<meta name="description" content="A blog about cooking and statistics.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="http://liyingbo.com/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="http://liyingbo.com/css/font-awesome.min.css">
<link rel="stylesheet" href="http://liyingbo.com/css/owl.carousel.css">
<link rel="stylesheet" href="http://liyingbo.com/css/owl.theme.css">


  <link href="http://liyingbo.com/css/style.blue.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="http://liyingbo.com/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="http://liyingbo.com/img/favicon.png">

  <link href="http://liyingbo.com/tags/introduction/index.xml" rel="alternate" type="application/rss+xml" title="King Fox And Butterfly" />


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-116913878-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    <h1 class="sidebar-heading"><a href="http://liyingbo.com">King Fox And Butterfly</a></h1>
    
      <p class="sidebar-p">I am a home cook and statistian, both with more than 10 years of experience.</p>
    
      <p class="sidebar-p">Originally from Shanghai, currently based in Dallas.</p>
    
    <ul class="sidebar-menu">
      
        <li><a href="http://liyingbo.com/cooking/">Cooking</a></li>
      
        <li><a href="http://liyingbo.com/stat/">Statistics</a></li>
      
        <li><a href="http://liyingbo.com/about/">About Me</a></li>
      
        <li><a href="http://liyingbo.com/categories/">Categories</a></li>
      
        <li><a href="http://liyingbo.com/tags/">Tags</a></li>
      
    </ul>
    <p class="social">
  
  
  
  
  
  
  <a href="https://www.linkedin.com/in/yingbo-li-08321723/" data-animate-hover="pulse" class="external">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/yingboli" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
</p>


    <div class="copyright">
      <p class="credit">
        
          &copy; Yingbo Li 2018 -- 2021 |
        
        Template by <a href="https://bootstrapious.com/free-templates" class="external">Bootstrapious.com</a>

&amp; ported to Hugo by <a href="https://github.com/kishaningithub">Kishan B</a>

      </p>
    </div>
  </div>
</div>

              
  <div class="col-xs-12 col-sm-8 col-md-9 content-column">
    <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left"> <i class="fa fa-align-left"> </i>Menu</button>
  <h1 class="small-navbar-heading"><a href="http://liyingbo.com">King Fox And Butterfly</a></h1>
</div>

    <div class="grid">
        <div class="row">
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-computer-age-statistical-inference-ch-15-multiple-testing/">Book Notes: Computer Age Statistical Inference -- Ch15 Multiple Testing</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Classical (Before Computer Age) Multiple Testing Corrections Bonferroni Correction Family-wise Error Rate  False Discovery Rates Benjamini-Hochberg FDR control An empirical Bayes view  Local False Discovery Rates Empirical Null   For the pdf slides, click here
Classical (Before Computer Age) Multiple Testing Corrections Background and notations  Before computer age, multiple testing may only involve 10 or 20 tests. With the emerge of biomedical (microarray) data, multiple testing may need to evaluate several thousands of tests
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-statistical-analysis-with-missing-data-ch3-complete-case-analysis-and-weighting-methods/">Book Notes: Statistical Analysis with Missing Data -- Ch3 Complete Case Analysis and Weighting Methods</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Weighted Complete-Case Analysis Available-Case Analysis   For the pdf slides, click here
Complete-case (CC) analysis  Complete-case (CC) analysis: use only data points (units) where all variables are observed
 Loss of information in CC analysis:
 Loss of precision (larger variance) Bias, when the missingness mechanism is not MCAR. In this case, the complete units are not a random sample of the population  In this notes, I will focus on the bias issue
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-flexible-imputation-of-missing-data-ch4-multivariate-missing-data/">Book Notes: Flexible Imputation of Missing Data -- Ch4 Multivariate Missing Data</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Missing Data Pattern Fully Conditional Specification (FCS)   For the pdf slides, click here
Notations in this chapter  \(Y\): a \(n \times p\) matrix which contains is missing data \(Y_j\): the \(j\)th column in \(Y\) \(Y_{-j}\): all but the \(j\)th column of \(Y\) \(R\): a \(n \times p\) missing indicator matrix  \(0\) is missing and \(1\) is observed    Missing Data Pattern Missing data pattern summary statistics  When the number of columns is small, we can use the md.
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-flexible-imputation-of-missing-data-ch3-univariate-missing-data/">Book Notes: Flexible Imputation of Missing Data -- Ch3 Univariate Missing Data</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Imputation under the Normal Linear Model Predictive Mean Matching Imputation under CART Imputing Categorical and Other Types of Data   For the pdf slides, click here
Notations  In this chapter, we assume that there is only one variable having missing values. We call this variable \(y\) the target variable.
 \(y_\text{obs}\): the \(n_1\) observed data in \(y\) \(y_\text{mis}\): the \(n_0\) missing data in \(y\) \(\dot{y}\): imputed values in \(y\)  Suppose \(X\) are the variables (covariates) in the imputation model.
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-flexible-imputation-of-missing-data-ch2-multiple-imputation/">Book Notes: Flexible Imputation of Missing Data -- Ch2 Multiple Imputation</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Concepts in Incomplete Data Why and When Multiple Imputation Works More about Imputation Methods   For the pdf slides, click here
Concepts in Incomplete Data Notations  \(m\): number of multiple imputations \(Y\): data of the sample  Includes both covariates and response Dimension \(n \times p\)  \(R\): observation indicator matrix, known  A \(n \times p\) 0-1 matrix \(r_{ij} =0\) for missing and 1 for observed  \(Y_{\text{obs}}\): observed data \(Y_{\text{mis}}\): missing data \(Y = (Y_{\text{obs}}, Y_{\text{mis}})\): complete data
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-flexible-imputation-of-missing-data-ch1-introduction/">Book Notes: Flexible Imputation of Missing Data -- Ch1 Introduction</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Concepts of MCAR, MAR, MNAR Ad-hoc Solutions Multiple Imputation in a Nutshell   For the pdf slides, click here
Concepts of MCAR, MAR, MNAR Concepts of MCAR, MAR, MNAR  Missing completely at random (MCAR): the probability of being missing is the same for all cases  Cause of missing is unrelated to the data  Missing at random (MAR): the probability of being missing only depends on the observed data  Cause of missing is unrelated to the missing values  Missing not at random (MNAR): probability of being missing depends on the missing values themselves    Ad-hoc Solutions Listwise deletion and pairwise deletion  Listwise deletion (also called complete-case analysis): delete rows which contain one or more missing values  If data is MCAR, listwise deletion produces unbiased estimates of means, variances, and regression weights (if need to train a predictive model) If data is not MCAR, listwise deletion can severely bias the above estimates.
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-computer-age-statistical-inference-ch-9-survival-analysis/">Book Notes: Computer Age Statistical Inference -- Ch9 Survival Analysis</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Survival Analysis Life Table and Kaplan-Meier Estimate Cox’s Proportional Hazards Model    For the pdf slides, click here
Survival Analysis Life Table and Kaplan-Meier Estimate Life table  An insurance company’s life table shows information of clients by their age. For each age \(i\), it contains
 \(n_i\): number of clients \(y_i\): number of death \(\hat{h}_i = y_i / n_i\): hazard rate \(\hat{S}_i\): survival probability estimate  An example life table
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-introduction-to-time-series-and-forecasting-ch3-arma-models/">Book Notes: Introduction to Time Series and Forecasting --  Ch3 ARMA Models</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  ARMA\((p, q)\) Processes Stationary solution Causality Invertibility  ACF and PACF of an ARMA\((p, q)\) Process Calculation of the ACVF Test for MAs and ARs from the ACF and PACF  Forecast ARMA Processes   For the pdf slides, click here
ARMA\((p, q)\) Processes ARMA\((p, q)\) process: definitions  \(\{X_t\}\) is an ARMA\((p, q)\) process if it is stationary, and for all \(t\), \[ X_t - \phi_1 X_{t-1} - \cdots - \phi_p X_{t-p} = Z_t + \theta_1 Z_{t-1} + \cdots + \theta_q Z_{t-q} \] where \(\{Z_t\} \sim \textrm{WN}(0, \sigma^2)\) and the polynomials \[ \phi(z) = 1 - \phi_1 z - \cdots - \phi_p z^p, \quad \theta(z) = 1 + \theta_1 z + \cdots + \theta_q z^q \] have no common factors
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-introduction-to-time-series-and-forecasting-ch2-stationary-processes/">Book Notes: Introduction to Time Series and Forecasting --  Ch2 Stationary Processes</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Linear Processes Introduction to ARMA Processes ARMA\((1,1)\) process  Properties of the Sample ACVF and Sample ACF Bartlett’s Formula  Forecast Stationary Time Series Best linear predictor: minimizes MSE Recursive methods: the Durbin-Levinson and Innovation Algorithms    For the pdf slides, click here
Best linear predictor  Goal: find a function of \(X_n\) that gives the “best” predictor of \(X_{n+h}\).
 We mean “best” by achieving minimum mean squared error Under joint normality assumption of \(X_n\) and \(X_{n+h}\), the best estimator is \[ m(X_n) = E(X_{n+h} \mid X_n) = \mu + \rho(h)(X_n - \mu) \]  Best linear predictor \[ \ell(X_n) = a X_n + b \]
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/book-notes-introduction-to-time-series-and-forecasting-ch1/">Book Notes: Introduction to Time Series and Forecasting --  Ch1 Introduction</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Stationary Models and Autocorrelation Function Examples of Simple Time Series Models  Estimate and Eliminate Trend and Seasonal Components Trend Component Only Also with the Seasonal Component  Test Whether Estimated Noises are IID   For the pdf slides, click here
Objective of time series models  Seasonal adjustment: recognize seasonal components and remove them to study long-term trends
 Separate (or filter) noise from signals
 Prediction
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
              <div class="col-xs-12 col-sm-6 col-md-4 col-lg-3 masonry-item">
                  <div class="box-masonry">
                      
                      
                      <div class="box-masonry-text">
                      
                          <h4><a href="http://liyingbo.com/stat/my-first-post-about-statistics/">My first post about statistics!</a></h4>
                          <div class="box-masonry-description">
                            <p>
                                
                                  Finally, a post not about cooking &hellip;
I recently gave a 1.5-hours lecture to friends who were interested in Bayesian statistics, and the slides are here.
This lecture covers (very briefly) the following topics
  Bayes formula (i.e., conditional probablity)
 Example: breast cancer screening    Bayesian parameter estimation (i.e., posterior distribution formula)
 Beta-Binomial updating, with a coin flipping example Example: Bayesian inference on the Rotten Tomato &ldquo;freshness&rdquo; of the Harry Potter movie series (This is what the above picture is about) MCMC (very high-level intro)    Bayesian hypothesis testing
                                
                            </p>
                          </div>
                      </div>
                  </div>
              </div>
          
        </div>
    </div>
</div>


          </div>
      </div>
  </div>
  <script src="http://liyingbo.com/js/jquery.min.js"></script>
<script src="http://liyingbo.com/js/bootstrap.min.js"></script>
<script src="http://liyingbo.com/js/jquery.cookie.js"> </script>
<script src="http://liyingbo.com/js/ekko-lightbox.js"></script>
<script src="http://liyingbo.com/js/jquery.scrollTo.min.js"></script>
<script src="http://liyingbo.com/js/masonry.pkgd.min.js"></script>
<script src="http://liyingbo.com/js/imagesloaded.pkgd.min.js"></script>
<script src="http://liyingbo.com/js/owl.carousel.min.js"></script>
<script src="http://liyingbo.com/js/front.js"></script>




<script>
    MathJax = {
        tex: {
            macros: {
                E: ['\\mathbf{E}'],
                V: ['\\mathbf{Var}'],
                C: ['\\mathbf{Cov}'],
                AV: ['\\mathbf{AVar}']
            }
        }
    };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</body>
</html>
