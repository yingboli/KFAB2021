---
title: 'Paper Notes: Generalized R Squared '
author: ''
date: '2020-10-05'
slug: paper-notes-generalized-r-squared
categories:
  - Paper notes
tags:
  - Generalized Linear Models
  - Slides
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
---


<div id="TOC">
<ul>
<li><a href="#generalized-r2-by-cox-and-snell">Generalized <span class="math inline">\(R^2\)</span> by Cox and Snell</a></li>
<li><a href="#generalized-r2-by-nagelkerke">Generalized <span class="math inline">\(R^2\)</span> by Nagelkerke</a><ul>
<li><a href="#generalized-r2-for-binary-data">Generalized <span class="math inline">\(R^2\)</span> for binary data</a></li>
</ul></li>
</ul>
</div>

<p><strong><em><font color='red'>For the pdf slides, click <a href="/pdf/100420_generalized_R2.pdf">here</a></font></em></strong></p>
<div id="r2-for-normal-linear-regression" class="section level3">
<h3><span class="math inline">\(R^2\)</span> for normal linear regression</h3>
<ul>
<li><p><span class="math inline">\(R^2\)</span>, also called <font color='blue'>coefficient of determination</font> or <font color='blue'>multiple correlation coefficient</font>,
is defined for normal linear regression,
as the proportion of variance “explained” by the regression model
<span class="math display">\[\begin{equation}\label{eq:R2}
R^2 = \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\sum_i \left( y_i - \bar{y} \right)^2}
\end{equation}\]</span></p></li>
<li><p>Note that under the MLE, where <span class="math inline">\(\hat{\sigma}^2 = \sum_i \left( y_i - \hat{y}_i \right)^2 / n\)</span>,
the deviance (i.e., negative two times log likelihood) is
<span class="math display">\[\begin{align*}
-2 l\left(\hat{\beta}\right) &amp; = -2 \log L(\hat{\beta})\\
&amp; = n \log(2\pi\hat{\sigma}^2) + \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{\hat{\sigma}^2}\\
&amp; = n \left[ \log\left( \frac{\sum_i \left( y_i - \hat{y}_i \right)^2}{n} \right) + \log(2\pi) + 1 \right]
\end{align*}\]</span></p>
<ul>
<li>I list this derivation here to make clear that the following generalized <span class="math inline">\(R^2\)</span> contains  as a special case for normal linear regression</li>
</ul></li>
</ul>
</div>
<div id="generalized-r2-by-cox-and-snell" class="section level1">
<h1>Generalized <span class="math inline">\(R^2\)</span> by Cox and Snell</h1>
<div id="generalized-r2-proposed-by-cox-and-snell-1989-and-also-magee-1990-and-maddala-1983" class="section level3">
<h3>Generalized <span class="math inline">\(R^2\)</span>, proposed by Cox and Snell [1989] (and also Magee [1990] and Maddala [1983])</h3>
<ul>
<li><p>The <font color='blue'>genralized <span class="math inline">\(R^2\)</span></font> for more general models where</p>
<ol style="list-style-type: decimal">
<li>the concept of residual variance cannot be easily define, and</li>
<li>maximum likelihood is the criterion of fit, is
<span class="math display">\[\begin{equation} \label{eq:generalized_R2_v1}
R^2 = 1 - \exp\left\{ -\frac{2}{n}\left[l\left(\hat{\beta}\right) - l(\hat{0})  \right] \right\} 
= 1 - \left[L(0)/L\left(\hat{\beta}\right)\right]^{2/n}
\end{equation}\]</span></li>
</ol></li>
<li><p>Here, <span class="math inline">\(L\left(\hat{\beta}\right)\)</span> and <span class="math inline">\(L(0)\)</span> are the likelihood of the fitted and the null models, respectively.</p></li>
<li><p>For normal linear regression, this generalized <span class="math inline">\(R^2\)</span>  becomes the classical <span class="math inline">\(R^2\)</span> </p></li>
</ul>
</div>
<div id="desirable-properties-of-the-generalized-r2-as-in-eq" class="section level3">
<h3>Desirable properties of the generalized <span class="math inline">\(R^2\)</span>, as in Eq </h3>
<ol style="list-style-type: decimal">
<li><p><font color='green'>Consistent with classical <span class="math inline">\(R^2\)</span></font></p></li>
<li><p><font color='green'>Consistent with maximum likelihood as an estimation method</font></p></li>
<li><p><font color='green'>Asymptotically independent of the sample size <span class="math inline">\(n\)</span></font></p></li>
<li><p><font color='green'><span class="math inline">\(1-R^2\)</span> has an interpretation as the propotion of unexplained “variation”</font></p>
<ul>
<li>For example, if we have three nested models, from smallest to largest, <span class="math inline">\(M_1, M_2\)</span>, and <span class="math inline">\(M_3\)</span>, then we have
<span class="math display">\[
 (1 - R^2_{3, 1}) = (1 - R^2_{3, 2})(1 - R^2_{2, 1})
 \]</span></li>
</ul></li>
</ol>
<ul>
<li>For more desirable properties (7 in total), please check out the Nagelkerke[1991] paper</li>
</ul>
</div>
</div>
<div id="generalized-r2-by-nagelkerke" class="section level1">
<h1>Generalized <span class="math inline">\(R^2\)</span> by Nagelkerke</h1>
<div id="generalized-r2-proposed-by-nagelkerke-1991" class="section level3">
<h3>Generalized <span class="math inline">\(R^2\)</span>, proposed by Nagelkerke [1991]</h3>
<ul>
<li><p><font color='red'>An undesirable property: for discrete models, the maximum <span class="math inline">\(R^2\)</span> is always less than 1</font>
<span class="math display">\[
\max(R^2) = 1 - L(0)^{2/n} 
\]</span></p>
<ul>
<li>This is because the likelihood of discrete target variables are from pmf (rather than from pdf, as of continuous targets)</li>
</ul></li>
<li><p><font color='blue'>A new definition of the generalized <span class="math inline">\(R^2\)</span></font>
<span class="math display">\[\begin{equation}\label{eq:generalized_R2_v2}
\bar{R}^2 = \frac{R^2}{\max(R^2)} 
= \frac{1 - \left[L(0)/L\left(\hat{\beta}\right)\right]^{2/n}}{1 - L(0)^{2/n}}
\end{equation}\]</span></p>
<ul>
<li><p>Majority of the desirable properties of , including the ones listed on the previous page, are still satisfied</p></li>
<li><p>Nagelkerke’s general <span class="math inline">\(R^2\)</span>  seems to be a popular version. For example, the biostat textbook by Steyerberg uses this version</p></li>
</ul></li>
</ul>
</div>
<div id="generalized-r2-for-binary-data" class="section level2">
<h2>Generalized <span class="math inline">\(R^2\)</span> for binary data</h2>
<div id="generalized-r2-for-binary-data-1" class="section level3">
<h3>Generalized <span class="math inline">\(R^2\)</span> for binary data</h3>
<ul>
<li><p>Denote the estimated binary probabilities as <span class="math inline">\(\hat{p}_i\)</span> for the fitted model, and <span class="math inline">\(\bar{p}\)</span> for the null model</p></li>
<li><p>Cox and Snell <span class="math inline">\(R^2\)</span>
<span class="math display">\[
R^2 
= 1 - \left[L(0)/L\left(\hat{\beta}\right)\right]^{2/n}
= 1 - \left[ \prod_i \left(\frac{\bar{p}}{\hat{p}_i}  \right)^{y_i} 
\left(\frac{1-\bar{p}}{1-\hat{p}_i}  \right)^{1-y_i}\right]^{2/n}
\]</span></p></li>
<li><p>Nagelkerke <span class="math inline">\(R^2\)</span>
<span class="math display">\[
\bar{R}^2 
= \frac{1 - \left[L(0)/L\left(\hat{\beta}\right)\right]^{2/n}}
{1 - L(0)^{2/n}}
= \frac{1 - \left[\prod_i \left(\frac{\bar{p}}{\hat{p}_i}  \right)^{y_i} 
\left(\frac{1-\bar{p}}{1-\hat{p}_i}  \right)^{1-y_i}\right]^{2/n}}
{1 - \left[\prod_i \bar{p}^{y_i} 
\left(1-\bar{p}\right)^{1-y_i}\right]^{2/n}}
\]</span></p></li>
</ul>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><p>Nagelkerke, N. J. D. (1991). A Note on a General Definition of the Coefficient of Determination. Biometrika, 78(3), 691-692.</p></li>
<li><p>A nice comparison of different versions of generalized <span class="math inline">\(R^2\)</span>: <a href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/" class="uri">https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/</a></p></li>
<li><p>Steyerberg, E. W. (2019). Clinical prediction models. Springer International Publishing.</p></li>
</ul>
</div>
</div>
</div>
