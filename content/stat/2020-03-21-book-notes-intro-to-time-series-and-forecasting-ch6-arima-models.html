---
title: 'Book Notes: Intro to Time Series and Forecasting -- Ch6 ARIMA Models'
author: ''
date: '2020-03-21'
slug: book-notes-intro-to-time-series-and-forecasting-ch6-arima-models
categories:
  - Book notes
tags:
  - Time Series
  - Slides
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
---


<div id="TOC">
<ul>
<li><a href="#arima-models">ARIMA Models</a><ul>
<li><a href="#transformation-and-identification-techniques">Transformation and Identification Techniques</a></li>
<li><a href="#unit-root-test">Unit Root Test</a></li>
<li><a href="#forecast-arima-models">Forecast ARIMA models</a></li>
</ul></li>
<li><a href="#seasonal-arima-models">Seasonal ARIMA Models</a></li>
<li><a href="#regression-with-arma-errors">Regression with ARMA Errors</a></li>
</ul>
</div>

<p><strong><em>For the pdf slides, click <a href="/pdf/012119_time_series_ch6.pdf">here</a></em></strong></p>
<div id="when-data-is-not-stationary" class="section level3">
<h3>When data is not stationary</h3>
<ul>
<li><p>Implication of not stationary: sample ACF or sample PACF
do not rapidly decrease to zero as lag increases</p></li>
<li>What shall we do?
<ul>
<li>Differencing, then fit an ARMA <span class="math inline">\(\rightarrow\)</span> ARIMA</li>
<li>Transformation, then fit an ARMA</li>
<li>Seasonal model <span class="math inline">\(\rightarrow\)</span> SARIMA</li>
</ul></li>
</ul>
</div>
<div id="a-non-stationary-exmaple-dow-jones-utilities-index-data" class="section level3">
<h3>A non-stationary exmaple: Dow Jones utilities index data</h3>
<pre class="r"><code>library(itsmr); ## Load the ITSM-R package
par(mfrow = c(1, 3));
plot.ts(dowj, main = &#39;Raw data&#39;); 
acf(dowj); pacf(dowj);</code></pre>
<p><img src="/stat/2020-03-21-book-notes-intro-to-time-series-and-forecasting-ch6-arima-models_files/figure-html/unnamed-chunk-1-1.png" width="864" /></p>
</div>
<div id="after-differencing" class="section level3">
<h3>After differencing</h3>
<pre class="r"><code>par(mfrow = c(1, 3));
dowj_diff = dowj[-length(dowj)] - dowj[-1];
plot.ts(dowj_diff, main = &#39;Data after differencing&#39;); 
acf(dowj_diff); pacf(dowj_diff);</code></pre>
<p><img src="/stat/2020-03-21-book-notes-intro-to-time-series-and-forecasting-ch6-arima-models_files/figure-html/unnamed-chunk-2-1.png" width="864" /></p>
</div>
<div id="arima-models" class="section level1">
<h1>ARIMA Models</h1>
<div id="arima-model-definition" class="section level3">
<h3>ARIMA model: definition</h3>
<ul>
<li><p><font color='blue'>Autoregressive integrated moving-average models (ARIMA)</font>:
Let <span class="math inline">\(d \in \mathbb{N}\)</span>, then series <span class="math inline">\(\{X_t\}\)</span> is an ARIMA<span class="math inline">\((p,d,q)\)</span> process if
<span class="math display">\[
Y_t = (1 - B)^d X_t
\]</span>
is a <strong>causal</strong> ARMA<span class="math inline">\((p,q)\)</span> process.</p></li>
<li><font color='blue'>Difference equation (DE)</font> for an ARIMA<span class="math inline">\((p,d,q)\)</span> process
<span class="math display">\[
\phi^*(B) X_t = \phi(B) (1-B)^d X_t = \theta(B) Z_t, \quad
\{Z_t\} \sim \textrm{WN}(0, \sigma^2)
\]</span>
<ul>
<li><span class="math inline">\(\phi(z)\)</span>: polynomial of degree <span class="math inline">\(p\)</span>, and <span class="math inline">\(\phi(z) \neq 0\)</span> for <span class="math inline">\(|z| \leq 1\)</span></li>
<li><span class="math inline">\(\theta(z)\)</span>: polynomial of degree <span class="math inline">\(q\)</span></li>
<li><span class="math inline">\(\phi^*(z) = \phi(z) (1-z)^d\)</span>: has a zero of order <span class="math inline">\(d\)</span> at <span class="math inline">\(z=1\)</span></li>
</ul></li>
<li><p><font color='red'>An ARIMA process with <span class="math inline">\(d &gt; 0\)</span> is NOT stationary!</font></p></li>
</ul>
</div>
<div id="arima-mean-is-not-dertermined-by-the-de" class="section level3">
<h3>ARIMA mean is not dertermined by the DE</h3>
<ul>
<li><p><span class="math inline">\(\{X_t\}\)</span> is an ARIMA<span class="math inline">\((p,d,q)\)</span> process. We can add an arbitrary polynomial
trend of degree <span class="math inline">\(d-1\)</span>
<span class="math display">\[
W_t = X_t + A_0 + A_1 t + \cdots + A_{d-1} t^{d-1}
\]</span>
with <span class="math inline">\(A_0, \ldots, A_{d-1}\)</span> being any random variables, and <span class="math inline">\(\{W_t\}\)</span>
still satisfies the same ARIMA<span class="math inline">\((p,d,q)\)</span> difference equation</p></li>
<li><p>In other words, the ARIMA DE determines the second-order properties of
<span class="math inline">\(\{(1-B)^d X_t\}\)</span> but not those of <span class="math inline">\(\{X_t\}\)</span></p>
<ul>
<li>For parameter estimation: <span class="math inline">\(\boldsymbol\phi\)</span>, <span class="math inline">\(\boldsymbol\theta\)</span>,
and <span class="math inline">\(\sigma^2\)</span> are estimated based on <span class="math inline">\(\{(1-B)^d X_t\}\)</span> rather than <span class="math inline">\(\{X_t\}\)</span></li>
<li>For forecast, we need additional assumptions</li>
</ul></li>
</ul>
</div>
<div id="fit-data-using-arima-processes" class="section level3">
<h3>Fit data using ARIMA processes</h3>
<ul>
<li>Whether to fit a finite time series using
<ul>
<li>non-stationary models (such as ARIMA), or</li>
<li>directly using stationary models (such as ARMA)?</li>
</ul></li>
<li>If the fitted stationary ARMA model’s <span class="math inline">\(\phi(\cdot)\)</span> have zeros very close
to unit circles, then fitting an ARIMA model is better
<ul>
<li>Parameter estimation is stable</li>
<li>The differenced series may only need a low-order ARMA</li>
</ul></li>
<li><font color='red'>Limitation of ARIMA</font>: only permits data to be nonstationary in a
very special way
<ul>
<li>Non-stationary: can have zeros anywhere on the unit circle <span class="math inline">\(|z|=1\)</span></li>
<li>ARIMA model: only has a zero of multiplicity <span class="math inline">\(d\)</span> at the point <span class="math inline">\(z=1\)</span></li>
</ul></li>
</ul>
</div>
<div id="transformation-and-identification-techniques" class="section level2">
<h2>Transformation and Identification Techniques</h2>
<div id="natural-log-transformation" class="section level3">
<h3>Natural log transformation</h3>
<ul>
<li><p>When data variance increases with mean, it’s common to apply log
transformation before fitting the data using ARIMA or ARMA.</p></li>
<li><p><font color='green'>When does log transfomation work well?</font> Suppose that
<span class="math display">\[
E(X_t) = \mu_t, \quad Var(X_t) = \sigma^2 \mu_t^2
\]</span>
Then by first-order Taylor expansion of <span class="math inline">\(\log(X_t)\)</span> at <span class="math inline">\(\mu_t\)</span>:
<span class="math display">\[
\log(X_t)  \approx \log(\mu_t) + \frac{X_t - \mu_t}{\mu_t} \ \Longrightarrow \ 
Var\left[\log(X_t)\right]  \approx \frac{Var(X_t)}{\mu_t^2} = \sigma^2
\]</span>
The data after log transformation <span class="math inline">\(\log(X_t)\)</span> has a constant variance</p></li>
<li>Note: log transformation can only be applied to positive data</li>
<li><p>Note: If <span class="math inline">\(Y_t = \log(X_t)\)</span>, then because expectation and logarithm are not
interchangeable,
<span class="math display">\[
\hat{X}_t \neq \exp(\hat{Y}_t)
\]</span></p></li>
</ul>
</div>
<div id="generalize-the-log-transformation-box-cox-transformation" class="section level3">
<h3>Generalize the log transformation: Box-Cox transformation</h3>
<ul>
<li><p><font color='blue'>Box-Cox transformation</font>
<span class="math display">\[
f_{\lambda}(x) = 
\begin{cases}
\frac{x^{\lambda} - 1}{\lambda}, &amp;  x \geq 0, \lambda &gt; 0 \\
\log(x), &amp;  x &gt; 0, \lambda = 0  \\
\end{cases}
\]</span></p>
<ul>
<li>Usual range: <span class="math inline">\(0 \leq \lambda \leq 1.5\)</span></li>
<li>Common values: <span class="math inline">\(\lambda = 0, 0.5\)</span></li>
</ul></li>
<li><p>Note: <span class="math inline">\(\lim_{\lambda \rightarrow 0} f_{\lambda}(x) = \log(x)\)</span></p></li>
<li><p>Box-Cox transformation can only be applied to non-negative data</p></li>
</ul>
</div>
</div>
<div id="unit-root-test" class="section level2">
<h2>Unit Root Test</h2>
<div id="unit-root-test-for-ar1-process" class="section level3">
<h3>Unit root test for AR<span class="math inline">\((1)\)</span> process</h3>
<ul>
<li><p><span class="math inline">\(\{X_t\}\)</span> is an AR<span class="math inline">\((1)\)</span> process: <span class="math inline">\(X_t - \mu = \phi_1(X_{t-1} - \mu) + Z_t\)</span></p></li>
<li>Equivalent DE:
<span class="math display">\[
\nabla X_t = X_t - X_{t-1} = \phi_0^* + \phi_1^* X_{t-1} + Z_t
\]</span>
<ul>
<li>where <span class="math inline">\(\phi_0^* = \mu(1 - \phi_1)\)</span> and <span class="math inline">\(\phi_1^* = \phi_1 - 1\)</span></li>
<li>Regressing <span class="math inline">\(\nabla X_t\)</span> onto <span class="math inline">\(1\)</span> and <span class="math inline">\(X_{t-1}\)</span>, we get the OLS estimator
<span class="math inline">\(\hat{\phi}_1^*\)</span> and its standard error <span class="math inline">\(SE(\hat{\phi}_1^*)\)</span></li>
</ul></li>
<li><font color='blue'>Augmented Dickey-Fuller test for AR<span class="math inline">\((1)\)</span></font>
<ul>
<li>Hypotheses: <span class="math inline">\(H_0: \phi_1 = 1 \ \longleftrightarrow \  H_1: \phi_1 &lt; 1\)</span></li>
<li>Equivalent hypotheses:
<span class="math inline">\(H_0: \phi_1^* = 0 \ \longleftrightarrow \ H_1: \phi_1^* &lt; 0\)</span></li>
<li>Test statistic: limit distribution under <span class="math inline">\(H_0\)</span> is not normal or t
<span class="math display">\[
\hat{\tau} = \frac{\hat{\phi}_1^*}{SE(\hat{\phi}_1^*)}
\]</span></li>
<li>Rejection region: reject <span class="math inline">\(H_0\)</span> if
<span class="math display">\[
\begin{cases}
\hat{\tau} &lt; -2.57 &amp; (90\%)\\
\hat{\tau} &lt; -2.86 &amp; (95\%)\\
\hat{\tau} &lt; -3.43 &amp; (99\%)
\end{cases}
\]</span></li>
</ul></li>
</ul>
</div>
<div id="unit-root-test-for-arp-process" class="section level3">
<h3>Unit root test for AR<span class="math inline">\((p)\)</span> process</h3>
<ul>
<li><p>AR<span class="math inline">\((p)\)</span> process:
<span class="math inline">\(X_t - \mu = \phi_1(X_{t-1} - \mu) + \cdots + \phi_p(X_{t-p} - \mu) + Z_t\)</span></p></li>
<li><p>Equivalent DE:
<span class="math display">\[
\nabla X_t = \phi_0^* + \phi_1^* X_{t-1} + \phi_2^* \nabla X_{t-1} + \cdots + \phi_p^* \nabla X_{t-p+1} + Z_t
\]</span></p>
<ul>
<li>where <span class="math inline">\(\phi_0^* = \mu(1 - \sum_{i=1}^p \phi_i)\)</span>,
<span class="math inline">\(\phi_1^* = \sum_{i=1}^p \phi_i - 1\)</span>, and
<span class="math inline">\(\phi_j^* = -\sum_{i=j}^p \phi_i\)</span> for <span class="math inline">\(j \geq 2\)</span></li>
<li>Regressing <span class="math inline">\(\nabla X_t\)</span> onto <span class="math inline">\(1, X_{t-1}, \nabla X_{t-1}, \cdots, \nabla X_{t-p+1}\)</span>, we get the OLS estimator
<span class="math inline">\(\hat{\phi}_1^*\)</span> and its standard error <span class="math inline">\(SE(\hat{\phi}_1^*)\)</span></li>
</ul></li>
<li><font color='blue'>Augmented Dickey-Fuller test for AR<span class="math inline">\((p)\)</span></font>
<ul>
<li>Hypotheses:
<span class="math inline">\(H_0: \phi_1^* = 0 \ \longleftrightarrow \ H_1: \phi_1^* &lt; 0\)</span></li>
<li>Test statistic:
<span class="math display">\[
\hat{\tau} = \frac{\hat{\phi}_1^*}{SE(\hat{\phi}_1^*)}
\]</span></li>
<li>Rejection region: same as augmented Dickey-Fuller test for AR<span class="math inline">\((1)\)</span></li>
</ul></li>
</ul>
</div>
<div id="implement-augmented-dickey-fuller-test-in-r" class="section level3">
<h3>Implement augmented Dickey-Fuller test in R</h3>
<pre class="r"><code>library(tseries);
## Note: the lag k here is the AR order p
adf.test(dowj, k = 2);</code></pre>
<pre><code>## 
##  Augmented Dickey-Fuller Test
## 
## data:  dowj
## Dickey-Fuller = -1.3788, Lag order = 2, p-value = 0.8295
## alternative hypothesis: stationary</code></pre>
</div>
</div>
<div id="forecast-arima-models" class="section level2">
<h2>Forecast ARIMA models</h2>
<div id="forecast-an-arimap-1-q-process" class="section level3">
<h3>Forecast an ARIMA<span class="math inline">\((p, 1, q)\)</span> process</h3>
<ul>
<li><p><span class="math inline">\(\{X_t\}\)</span> is ARIMA<span class="math inline">\((p, 1, q)\)</span>, and <span class="math inline">\(\{Y_t = \nabla X_t\}\)</span> is a causal
ARMA<span class="math inline">\((p, q)\)</span>
<span class="math display">\[
X_t = X_0 + \sum_{j=1}^t Y_j, \quad t = 1, 2, \ldots
\]</span></p></li>
<li>Best linear predictor of <span class="math inline">\(X_{n+1}\)</span>
<span class="math display">\[
P_n X_{n+1} = P_n(X_0 + Y_1 + \cdots + Y_{n+1}) = P_n(X_n + Y_{n+1}) 
= X_n + P_n(Y_{n+1}),
\]</span>
<ul>
<li><span class="math inline">\(P_n\)</span> means based on <span class="math inline">\(\{1, X_0, X_1, \ldots, X_n\}\)</span>, or equivalently,
<span class="math inline">\(\{1, X_0, Y_1, \ldots, Y_n\}\)</span></li>
<li>To find <span class="math inline">\(P_n(Y_{n+1})\)</span>, we need to know <span class="math inline">\(E(X_0^2)\)</span> and
<span class="math inline">\(E(X_0 Y_j)\)</span>, for <span class="math inline">\(j = 1, \ldots, n+1\)</span>.</li>
</ul></li>
<li><p><font color='red'>A sufficient assumption</font> for <span class="math inline">\(P_n(Y_{n+1})\)</span> to be the best linear
predictor in term of <span class="math inline">\(\{Y_1, \ldots, Y_n\}\)</span>:
<span class="math inline">\(X_0\)</span> is uncorrelated with <span class="math inline">\(Y_1, Y_2, \ldots\)</span></p></li>
</ul>
</div>
<div id="forecast-an-arimap-d-q-process" class="section level3">
<h3>Forecast an ARIMA<span class="math inline">\((p, d, q)\)</span> process</h3>
<ul>
<li><p>The observed ARIMA<span class="math inline">\((p, d, q)\)</span> process <span class="math inline">\(\{X_t\}\)</span> satisfies
<span class="math display">\[
Y_t = (1-B)^d X_t, \quad t = 1, 2, \ldots, \quad \{Y_t\} \sim \text{causal ARMA}(p, q)
\]</span></p></li>
<li><p><font color='red'>Assumption</font>: the random vector <span class="math inline">\((X_{1-d}, \ldots, X_0)\)</span> is
uncorrelated with <span class="math inline">\(Y_t\)</span> for all <span class="math inline">\(t &gt; 0\)</span></p></li>
<li><p>One-step predictors <span class="math inline">\(\hat{Y}_{n+1} = P_n Y_{n+1}\)</span> and
<span class="math inline">\(\hat{X}_{n+1} = P_n X_{n+1}\)</span>:
<span class="math display">\[
X_{n+1} - \hat{X}_{n+1} = Y_{n+1} - \hat{Y}_{n+1}
\]</span></p></li>
<li><p>Recall: the <span class="math inline">\(h\)</span>-step predictor of ARMA<span class="math inline">\((p,q)\)</span> for <span class="math inline">\(n &gt; \max(p, q)\)</span>:
<span class="math display">\[
P_n Y_{n+h} = \sum_{i=1}^p \phi_i P_n Y_{n+h-i} + 
\sum_{j=h}^q \theta_{n+h-1, j}(Y_{n+h-j} - \hat{Y}_{n+h-j})
\]</span></p></li>
<li><p><span class="math inline">\(h\)</span>-step predictor of ARIMA<span class="math inline">\((p, d, q)\)</span> for <span class="math inline">\(n &gt; \max(p, q)\)</span>:
<span class="math display">\[
P_n X_{n+h} = \sum_{i=1}^{p+d} \phi_i^* P_n X_{n+h-i} + 
\sum_{j=h}^q \theta_{n+h-1, j}(X_{n+h-j} - \hat{X}_{n+h-j})
\]</span>
where <span class="math inline">\(\phi^*(z) = (1-z)^d \phi(z) = 1 - \phi_1^*z - \cdots -\phi_{p+d}^* z^{p+d}\)</span></p></li>
</ul>
</div>
</div>
</div>
<div id="seasonal-arima-models" class="section level1">
<h1>Seasonal ARIMA Models</h1>
<div id="seasonal-arima-sarima-model-definition" class="section level3">
<h3>Seasonal ARIMA (SARIMA) Model: definition</h3>
<ul>
<li><p>Suppose <span class="math inline">\(d, D\)</span> are non-negative integers. <span class="math inline">\(\{X_t\}\)</span> is a
<font color='blue'>seasonal ARIMA<span class="math inline">\((p, d, q)\)</span></font> <span class="math inline">\(\times\)</span> <font color='blue'><span class="math inline">\((P, D, Q)_s\)</span>
process with period <span class="math inline">\(s\)</span></font> if the differenced series
<span class="math display">\[
Y_t = (1-B)^d (1-B^s)^D X_t
\]</span>
is a causal ARMA process defined by
<span class="math display">\[
\phi(B) \Phi(B^s) Y_t = \theta(B) \Theta(B^s) Z_t, \quad 
\{Z_t\} \sim \textrm{WN}(0, \sigma^2) 
\]</span>
where
<span class="math display">\[\begin{align*}
\phi(z) &amp; = 1 - \phi_1 z - \cdots - \phi_p z^p, \quad
\Phi(z)  = 1 - \Phi_1 z - \cdots - \Phi_P z^P \\
\theta(z) &amp; = 1 + \theta z + \cdots + \theta_q z^q, \quad
\Theta(z)  = 1 + \Theta z + \cdots + \Theta_Q z^Q
\end{align*}\]</span></p></li>
<li><p><span class="math inline">\(\{Y_t\}\)</span> is causal if and only if neither <span class="math inline">\(\phi(z)\)</span> or <span class="math inline">\(\Phi(z)\)</span> has zeros
inside the unit circle</p></li>
<li><p>Usually, <span class="math inline">\(s=12\)</span> for monthly data</p></li>
</ul>
</div>
<div id="special-case-seasonal-arma-sarma" class="section level3">
<h3>Special case: seasonal ARMA (SARMA)</h3>
<ul>
<li><p><font color='blue'>Between-year model</font>: for monthly data,
each one of the 12 time series is generated by the same
ARMA<span class="math inline">\((P, Q)\)</span> model
<span class="math display">\[
\Phi(B^{12}) Y_t = \Theta(B^{12}) U_t, \quad \{U_{j+12t}, t \in \mathbb{Z}\}
\sim \textrm{WN}(0, \sigma^2_U)
\]</span></p></li>
<li><font color='blue'>SARMA<span class="math inline">\((P, Q)\)</span> with period <span class="math inline">\(s\)</span></font>:
in the above between-year model, the period 12
can be changed to any positive integer <span class="math inline">\(s\)</span>
<ul>
<li>If <span class="math inline">\(\{U_{t}, t \in \mathbb{Z}\}\sim \textrm{WN}(0, \sigma^2_U)\)</span>, then
the ACVF <span class="math inline">\(\gamma(h) = 0\)</span> unless <span class="math inline">\(h\)</span> divides <span class="math inline">\(s\)</span> evenly.
But this may not be ideal for real life application!
E.g., this Feb is correlated with last Feb, but not this Jan.</li>
</ul></li>
<li><font color='blue'>General SARMA<span class="math inline">\((p, q)\)</span></font> <span class="math inline">\(\times\)</span> <font color='blue'><span class="math inline">\((P, Q)\)</span> with period <span class="math inline">\(s\)</span></font>:
incorporate dependency between the 12 series by letting <span class="math inline">\(\{U_t\}\)</span> be ARMA:
<span class="math display">\[
\Phi(B^{s}) Y_t = \Theta(B^{s}) U_t, \quad 
\phi(B) U_t = \theta(B)Z_t, \quad 
\{Z_t\} \sim \textrm{WN}(0, \sigma^2)
\]</span>
<ul>
<li>Equivalent DE for the general SARMA:
<span class="math display">\[
  \phi(B) \Phi(B^s) Y_t = \theta(B) \Theta(B^s) Z_t
  \]</span></li>
</ul></li>
</ul>
</div>
<div id="fit-a-sarima-model" class="section level3">
<h3>Fit a SARIMA Model</h3>
<ul>
<li>Period <span class="math inline">\(s\)</span> is known</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Find <span class="math inline">\(d\)</span> and <span class="math inline">\(D\)</span> to make the difference series <span class="math inline">\(\{Y_t\}\)</span> to look stationary</p></li>
<li><p>Examine the sample ACF and sample PACF of <span class="math inline">\(\{Y_t\}\)</span> at lags being
multiples of <span class="math inline">\(s\)</span>, to find orders <span class="math inline">\(P, Q\)</span></p></li>
<li><p>Use <span class="math inline">\(\hat{\rho}(1), \ldots, \hat{\rho}(s-1)\)</span> to find orders <span class="math inline">\(p, q\)</span></p></li>
<li><p>Use AICC to decide among competing order choices</p></li>
<li><p>Given orders <span class="math inline">\((p, d, q, P, D, Q)\)</span>, estimate MLE of parameters <span class="math inline">\((\phi, \theta, \Phi, \Theta, \sigma^2)\)</span></p></li>
</ol>
</div>
</div>
<div id="regression-with-arma-errors" class="section level1">
<h1>Regression with ARMA Errors</h1>
<div id="regression-with-arma-errors-ols-estimation" class="section level3">
<h3>Regression with ARMA errors: OLS estimation</h3>
<ul>
<li>Linear model with ARMA errors <span class="math inline">\(\mathbf{W} = (W_1, \ldots, W_n)&#39;\)</span>:
<span class="math display">\[
Y_t = \mathbf{x}_t&#39; \boldsymbol\beta + W_t, \quad t = 1, \ldots, n, \quad
\{W_t\} \sim \text{causal ARMA}(p,q)
\]</span>
<ul>
<li>Note: each row is indexed by a different time <span class="math inline">\(t\)</span>!</li>
<li>Error covariance <span class="math inline">\(\boldsymbol\Gamma_n = E(\mathbf{W} \mathbf{W}&#39;)\)</span></li>
</ul></li>
<li><font color='blue'>Ordinary least squares (OLS) estimator</font>
<span class="math display">\[
\hat{\boldsymbol\beta}_{\text{OLS}} =
(\mathbf{X}&#39;\mathbf{X})^{-1}\mathbf{X}&#39;\mathbf{Y}
\]</span>
<ul>
<li>Estimated by minimizing
<span class="math inline">\((\mathbf{Y} - \mathbf{X}\boldsymbol\beta)&#39;  (\mathbf{Y} - \mathbf{X}\boldsymbol\beta)\)</span></li>
<li><font color='red'>OLS is unbiased</font>, even when errors are dependent!</li>
</ul></li>
</ul>
</div>
<div id="regression-with-arma-errors-gls-estimation" class="section level3">
<h3>Regression with ARMA errors: GLS estimation</h3>
<ul>
<li><font color='blue'>Generalized least squares (GLS) estimator</font>
<span class="math display">\[
\hat{\boldsymbol\beta}_{\text{GLS}} =
(\mathbf{X}&#39;\boldsymbol\Gamma_n^{-1}\mathbf{X})^{-1}\mathbf{X}&#39;\boldsymbol\Gamma_n^{-1}\mathbf{Y}
\]</span>
<ul>
<li><p>Estimated by minimizing the weighted sum of squares
<span class="math display">\[
  (\mathbf{Y} - \mathbf{X}\boldsymbol\beta)&#39; \boldsymbol\Gamma_n^{-1}
  (\mathbf{Y} - \mathbf{X}\boldsymbol\beta)
  \]</span></p></li>
<li><p>Covariance
<span class="math display">\[
\textrm{Cov}\left( \hat{\boldsymbol\beta}_{\text{GLS}} \right) = 
(\mathbf{X}&#39;\boldsymbol\Gamma_n^{-1}\mathbf{X})^{-1}
\]</span></p></li>
<li><p><font color='red'>GLS is the best linear unbiased estimator</font>, i.e., for any
vector <span class="math inline">\(\mathbf{c}\)</span> and any unbiased estimator <span class="math inline">\(\hat{\boldsymbol\beta}\)</span>,
we have
<span class="math display">\[
\textrm{Var}(\mathbf{c}&#39; \hat{\boldsymbol\beta}_{\text{GLS}}) \leq
\textrm{Var}(\mathbf{c}&#39; \hat{\boldsymbol\beta})
\]</span></p></li>
</ul></li>
</ul>
</div>
<div id="when-w_t-is-an-arp-process" class="section level3">
<h3>When <span class="math inline">\(\{W_t\}\)</span> is an AR<span class="math inline">\((p)\)</span> process</h3>
<ul>
<li><p>We can apply <span class="math inline">\(\phi(B)\)</span> to each side of the regression equation and get
uncorrelated, zero-mean, constant-variance errors
<span class="math display">\[
\phi(B) \mathbf{Y} = \phi(B) \mathbf{X} \boldsymbol\beta + \phi(B) \mathbf{W} = \phi(B) \mathbf{X} \boldsymbol\beta + \mathbf{Z}
\]</span></p></li>
<li><p>Under the transformed target variable
<span class="math display">\[
Y_t^* = \phi(B) Y_t, \quad t = p+1, \ldots, n
\]</span>
and the transformed design matrix
<span class="math display">\[
X_{t, j}^* = \phi(B) X_{t, j}, \quad t = p+1, \ldots, n, \quad j = 1, \ldots, k
\]</span>
the OLS estimator is the best linear unbiased estimator</p></li>
<li><p>Note: after the transformation, the regression sample size reduces to <span class="math inline">\(n-p\)</span></p></li>
</ul>
</div>
<div id="regression-with-arma-errors-mle" class="section level3">
<h3>Regression with ARMA errors: MLE</h3>
<ul>
<li><p>MLE of <span class="math inline">\(\boldsymbol\beta, \boldsymbol\phi, \boldsymbol\theta, \sigma^2\)</span>
can be estimated by maximizing the Gaussian likelihood with error
covariance <span class="math inline">\(\boldsymbol\Gamma_n\)</span></p></li>
<li><p>An iterative scheme</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(\hat{\boldsymbol\beta}_{\text{OLS}}\)</span> and regression residuals
<span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol\beta}_{\text{OLS}}\)</span></p></li>
<li><p>Based on the estimated residuals, compute MLE of the ARMA<span class="math inline">\((p, q)\)</span> parameters</p></li>
<li><p>Based on the fitted ARMA model, compute <span class="math inline">\(\hat{\boldsymbol\beta}_{\text{GLS}}\)</span></p></li>
<li><p>Compute regression residuals
<span class="math inline">\(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol\beta}_{\text{GLS}}\)</span>, and return to
Step 2 until estimators stabilize</p></li>
</ol></li>
<li>Asymptotic properties of MLE:
If <span class="math inline">\(\{W_t\}\)</span> is a causal and invertible ARMA, then
<ul>
<li>MLEs are asymptotically normal</li>
<li>Estimated regression coefficients are asymptotically independent of
estimated ARMA parameters</li>
</ul></li>
</ul>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><p>Brockwell, Peter J. and Davis, Richard A. (2016), <em>Introduction to Time Series and Forecasting, Third Edition</em>. New York: Springer</p></li>
<li><p>Weigt, George (2018) <em>ITSM-R Reference Manual</em> <a href="http://www.eigenmath.org/itsmr-refman.pdf" class="uri">http://www.eigenmath.org/itsmr-refman.pdf</a></p></li>
<li><p>R package <code>tseries</code>
<a href="https://cran.r-project.org/web/packages/tseries/index.html" class="uri">https://cran.r-project.org/web/packages/tseries/index.html</a></p></li>
</ul>
</div>
</div>
